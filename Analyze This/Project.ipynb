{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "train_df = pd.read_csv(\"development_dataset.csv\",index_col=0)\n",
    "leader_df = pd.read_csv(\"leaderboard_dataset.csv\",index_col=0)\n",
    "evaluation_df = pd.read_csv(\"Evaluation_dataset.csv\",index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VAR2</th>\n",
       "      <th>VAR3</th>\n",
       "      <th>VAR4</th>\n",
       "      <th>VAR5</th>\n",
       "      <th>VAR6</th>\n",
       "      <th>VAR7</th>\n",
       "      <th>VAR8</th>\n",
       "      <th>VAR9</th>\n",
       "      <th>VAR10</th>\n",
       "      <th>VAR11</th>\n",
       "      <th>VAR12</th>\n",
       "      <th>VAR13</th>\n",
       "      <th>VAR14</th>\n",
       "      <th>VAR15</th>\n",
       "      <th>VAR16</th>\n",
       "      <th>VAR17</th>\n",
       "      <th>VAR18</th>\n",
       "      <th>VAR19</th>\n",
       "      <th>VAR20</th>\n",
       "      <th>VAR21</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VAR1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>828.235294</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>321.428571</td>\n",
       "      <td>625.911006</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>1.081550</td>\n",
       "      <td>198.113469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>0.279273</td>\n",
       "      <td>100.083403</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.540594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100.104991</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>911.764706</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>8.181818</td>\n",
       "      <td>1.344479</td>\n",
       "      <td>198.600020</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.012510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.614613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>146.654045</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>962.352941</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>615.825381</td>\n",
       "      <td>8.181818</td>\n",
       "      <td>0.720796</td>\n",
       "      <td>197.267767</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>0.453212</td>\n",
       "      <td>210.175146</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.044599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98.249570</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>892.941177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>638.076431</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>0.820218</td>\n",
       "      <td>197.355744</td>\n",
       "      <td>4.363431</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.145729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140.862306</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>914.117647</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>626.514988</td>\n",
       "      <td>5.181818</td>\n",
       "      <td>1.372928</td>\n",
       "      <td>198.790477</td>\n",
       "      <td>85.938202</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>210.175146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.558341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>101.268503</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>794.117647</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>178.571429</td>\n",
       "      <td>624.065797</td>\n",
       "      <td>12.727273</td>\n",
       "      <td>0.665458</td>\n",
       "      <td>197.251700</td>\n",
       "      <td>107.066619</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>0.139290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.037051</td>\n",
       "      <td>34.83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>379.736661</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>987.058823</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.970320</td>\n",
       "      <td>197.831747</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.446909</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.161463</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>146.382100</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>921.176471</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>612.660165</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>0.585291</td>\n",
       "      <td>196.956773</td>\n",
       "      <td>71.123926</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>0.905039</td>\n",
       "      <td>80.066722</td>\n",
       "      <td>5.133333</td>\n",
       "      <td>1</td>\n",
       "      <td>5.133333</td>\n",
       "      <td>0.975338</td>\n",
       "      <td>894.33</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>103.776248</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>947.058823</td>\n",
       "      <td>NaN</td>\n",
       "      <td>821.428571</td>\n",
       "      <td>648.330121</td>\n",
       "      <td>23.636364</td>\n",
       "      <td>0.782048</td>\n",
       "      <td>200.301604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>2.886284</td>\n",
       "      <td>20.016681</td>\n",
       "      <td>9.733333</td>\n",
       "      <td>1</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.141355</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96.337349</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>940.000000</td>\n",
       "      <td>0.137778</td>\n",
       "      <td>107.142857</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>0.559710</td>\n",
       "      <td>196.911295</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>47.899686</td>\n",
       "      <td>4.302053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.959315</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113.433735</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            VAR2      VAR3        VAR4        VAR5       VAR6      VAR7  \\\n",
       "VAR1                                                                      \n",
       "1     828.235294  0.138889  321.428571  625.911006   1.818182  1.081550   \n",
       "2     911.764706  0.027778         NaN  611.574748   8.181818  1.344479   \n",
       "3     962.352941  0.833333   35.714286  615.825381   8.181818  0.720796   \n",
       "4     892.941177       NaN         NaN  638.076431   9.090909  0.820218   \n",
       "5     914.117647  0.083333         NaN  626.514988   5.181818  1.372928   \n",
       "6     794.117647  0.117700  178.571429  624.065797  12.727273  0.665458   \n",
       "7     987.058823       NaN   35.714286  611.574748        NaN  0.970320   \n",
       "8     921.176471  0.138889         NaN  612.660165  18.181818  0.585291   \n",
       "9     947.058823       NaN  821.428571  648.330121  23.636364  0.782048   \n",
       "10    940.000000  0.137778  107.142857  611.574748   9.090909  0.559710   \n",
       "\n",
       "            VAR8        VAR9      VAR10     VAR11       VAR12      VAR13  \\\n",
       "VAR1                                                                       \n",
       "1     198.113469         NaN  58.632548  0.279273  100.083403   1.000000   \n",
       "2     198.600020   22.086661        NaN       NaN   15.012510        NaN   \n",
       "3     197.267767         NaN  58.632548  0.453212  210.175146  10.000000   \n",
       "4     197.355744    4.363431  58.632548       NaN         NaN        NaN   \n",
       "5     198.790477   85.938202  58.632548       NaN  210.175146        NaN   \n",
       "6     197.251700  107.066619  58.632548  0.139290         NaN   2.000000   \n",
       "7     197.831747   22.086661        NaN  3.446909         NaN        NaN   \n",
       "8     196.956773   71.123926  58.632548  0.905039   80.066722   5.133333   \n",
       "9     200.301604         NaN  58.632548  2.886284   20.016681   9.733333   \n",
       "10    196.911295   22.086661  47.899686  4.302053         NaN   9.200000   \n",
       "\n",
       "     VAR14      VAR15     VAR16    VAR17  VAR18  VAR19       VAR20   VAR21  \n",
       "VAR1                                                                        \n",
       "1        1   1.000000  1.540594      NaN      1      0  100.104991     Low  \n",
       "2        1        NaN  1.614613      NaN      0      1  146.654045    High  \n",
       "3        1  10.000000  1.044599      NaN      0      0   98.249570  Medium  \n",
       "4        1        NaN  1.145729      NaN      1      0  140.862306     Low  \n",
       "5        .        NaN  1.558341      NaN      1      0  101.268503    High  \n",
       "6        1   2.000000  1.037051    34.83      0      0  379.736661     Low  \n",
       "7        1   6.000000  1.161463  1000.00      0      1  146.382100     Low  \n",
       "8        1   5.133333  0.975338   894.33      1      0  103.776248  Medium  \n",
       "9        1  11.000000  1.141355  1000.00      1      0   96.337349     Low  \n",
       "10       1   9.200000  0.959315  1000.00      1      0  113.433735  Medium  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAR2      2.826471\n",
       "VAR3      9.955882\n",
       "VAR4     36.747059\n",
       "VAR5      0.002941\n",
       "VAR6     10.279412\n",
       "VAR7      2.455882\n",
       "VAR8      2.455882\n",
       "VAR9     46.520588\n",
       "VAR10    13.382353\n",
       "VAR11    22.250000\n",
       "VAR12    30.888235\n",
       "VAR13    25.064706\n",
       "VAR14     0.000000\n",
       "VAR15    23.600000\n",
       "VAR16     2.547059\n",
       "VAR17    65.276471\n",
       "VAR18     0.000000\n",
       "VAR19     0.000000\n",
       "VAR20     0.000000\n",
       "VAR21     0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()/340"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAR2     float64\n",
       "VAR3     float64\n",
       "VAR4     float64\n",
       "VAR5     float64\n",
       "VAR6     float64\n",
       "VAR7     float64\n",
       "VAR8     float64\n",
       "VAR9     float64\n",
       "VAR10    float64\n",
       "VAR11    float64\n",
       "VAR12    float64\n",
       "VAR13    float64\n",
       "VAR14     object\n",
       "VAR15    float64\n",
       "VAR16    float64\n",
       "VAR17    float64\n",
       "VAR18      int64\n",
       "VAR19      int64\n",
       "VAR20    float64\n",
       "VAR21     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.replace('.', 'nan')\n",
    "train_df['VAR14'] = train_df['VAR14'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAR2     float64\n",
       "VAR3     float64\n",
       "VAR4     float64\n",
       "VAR5     float64\n",
       "VAR6     float64\n",
       "VAR7     float64\n",
       "VAR8     float64\n",
       "VAR9     float64\n",
       "VAR10    float64\n",
       "VAR11    float64\n",
       "VAR12    float64\n",
       "VAR13    float64\n",
       "VAR14    float64\n",
       "VAR15    float64\n",
       "VAR16    float64\n",
       "VAR17    float64\n",
       "VAR18      int64\n",
       "VAR19      int64\n",
       "VAR20    float64\n",
       "VAR21     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2=train_df.loc[:,:'VAR20']\n",
    "train_df2['source']='train_df2'\n",
    "leader_df['source']='leader_df'\n",
    "evaluation_df['source']='evaluation_df'\n",
    "data1 = pd.concat([train_df2, leader_df ],ignore_index=True)\n",
    "data1 = pd.concat([data1, evaluation_df ],ignore_index=True)\n",
    "data1 = data1.replace('.', 'nan')\n",
    "\n",
    "data1['VAR14'] = data1['VAR14'].astype(float)\n",
    "\n",
    "import sys\n",
    "from impyute.imputation.cs import fast_knn\n",
    "#data = train_df.loc[:,:'VAR20']\n",
    "# start the KNN training\n",
    "imputed_training=fast_knn(data1.loc[:, :'VAR20'].values, k=5)\n",
    "\n",
    "imputed_saved=imputed_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x285c3d24ec8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imputed_training=pd.DataFrame(imputed_training)\n",
    "corr = imputed_training.corr()\n",
    "#import seaborn as sns\n",
    "sns.heatmap(corr)\n",
    "\n",
    "#data = train_df.loc[:,:'VAR20']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>828.235294</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>321.428571</td>\n",
       "      <td>625.911006</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>198.113469</td>\n",
       "      <td>54.142628</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>0.279273</td>\n",
       "      <td>100.083403</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.540594</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.104991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>911.764706</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>247.070921</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>8.181818</td>\n",
       "      <td>198.600020</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>56.802571</td>\n",
       "      <td>4.706554</td>\n",
       "      <td>15.012510</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.684988</td>\n",
       "      <td>1.614613</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>146.654045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>962.352941</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>615.825381</td>\n",
       "      <td>8.181818</td>\n",
       "      <td>197.267767</td>\n",
       "      <td>54.777565</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>0.453212</td>\n",
       "      <td>210.175146</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.044599</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.249570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>892.941177</td>\n",
       "      <td>0.893821</td>\n",
       "      <td>247.070921</td>\n",
       "      <td>638.076431</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>197.355744</td>\n",
       "      <td>4.363431</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>5.424166</td>\n",
       "      <td>116.998948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.509678</td>\n",
       "      <td>1.145729</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.862306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>914.117647</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>248.287889</td>\n",
       "      <td>626.514988</td>\n",
       "      <td>5.181818</td>\n",
       "      <td>198.790477</td>\n",
       "      <td>85.938202</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>2.877702</td>\n",
       "      <td>210.175146</td>\n",
       "      <td>1.830956</td>\n",
       "      <td>11.408517</td>\n",
       "      <td>1.558341</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.268503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53995</td>\n",
       "      <td>961.176471</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>247.070921</td>\n",
       "      <td>617.241675</td>\n",
       "      <td>9.878793</td>\n",
       "      <td>197.148867</td>\n",
       "      <td>32.832680</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>0.614678</td>\n",
       "      <td>116.998948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.998794</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107.493976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53996</td>\n",
       "      <td>823.529412</td>\n",
       "      <td>0.493867</td>\n",
       "      <td>247.070921</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>198.066831</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>56.928321</td>\n",
       "      <td>5.424166</td>\n",
       "      <td>116.998948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.509678</td>\n",
       "      <td>0.970848</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>170.817556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53997</td>\n",
       "      <td>931.764706</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>198.660971</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>49.793908</td>\n",
       "      <td>0.246010</td>\n",
       "      <td>15.012510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>1.328066</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>114.246127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53998</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>197.223243</td>\n",
       "      <td>0.924021</td>\n",
       "      <td>58.331180</td>\n",
       "      <td>0.503800</td>\n",
       "      <td>116.998948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.266667</td>\n",
       "      <td>1.075421</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>117.898451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53999</td>\n",
       "      <td>847.058823</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>622.264355</td>\n",
       "      <td>2.727273</td>\n",
       "      <td>197.742633</td>\n",
       "      <td>0.256672</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>1.561296</td>\n",
       "      <td>40.033361</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>1.086612</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>118.464716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54000 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1           2           3          4           6   \\\n",
       "0       828.235294  0.138889  321.428571  625.911006   1.818182  198.113469   \n",
       "1       911.764706  0.027778  247.070921  611.574748   8.181818  198.600020   \n",
       "2       962.352941  0.833333   35.714286  615.825381   8.181818  197.267767   \n",
       "3       892.941177  0.893821  247.070921  638.076431   9.090909  197.355744   \n",
       "4       914.117647  0.083333  248.287889  626.514988   5.181818  198.790477   \n",
       "...            ...       ...         ...         ...        ...         ...   \n",
       "53995   961.176471  0.055556  247.070921  617.241675   9.878793  197.148867   \n",
       "53996   823.529412  0.493867  247.070921  611.574748  50.000000  198.066831   \n",
       "53997   931.764706  0.027778   35.714286  611.574748   2.272727  198.660971   \n",
       "53998  1000.000000  0.033333   35.714286  611.574748   9.090909  197.223243   \n",
       "53999   847.058823  0.138889   35.714286  622.264355   2.727273  197.742633   \n",
       "\n",
       "              7          8         9           10        12         13  \\\n",
       "0      54.142628  58.632548  0.279273  100.083403  1.000000   1.000000   \n",
       "1      22.086661  56.802571  4.706554   15.012510  1.000000  12.684988   \n",
       "2      54.777565  58.632548  0.453212  210.175146  1.000000  10.000000   \n",
       "3       4.363431  58.632548  5.424166  116.998948  1.000000  12.509678   \n",
       "4      85.938202  58.632548  2.877702  210.175146  1.830956  11.408517   \n",
       "...          ...        ...       ...         ...       ...        ...   \n",
       "53995  32.832680  58.632548  0.614678  116.998948  1.000000   0.333333   \n",
       "53996  22.086661  56.928321  5.424166  116.998948  1.000000  12.509678   \n",
       "53997  22.086661  49.793908  0.246010   15.012510  0.000000   5.600000   \n",
       "53998   0.924021  58.331180  0.503800  116.998948  1.000000   7.266667   \n",
       "53999   0.256672  58.632548  1.561296   40.033361  2.000000   5.333333   \n",
       "\n",
       "             14          15   16   17          18  \n",
       "0      1.540594  820.136775  1.0  0.0  100.104991  \n",
       "1      1.614613  820.136775  0.0  1.0  146.654045  \n",
       "2      1.044599  820.136775  0.0  0.0   98.249570  \n",
       "3      1.145729  820.136775  1.0  0.0  140.862306  \n",
       "4      1.558341  820.136775  1.0  0.0  101.268503  \n",
       "...         ...         ...  ...  ...         ...  \n",
       "53995  0.998794  820.136775  0.0  0.0  107.493976  \n",
       "53996  0.970848  820.136775  0.0  1.0  170.817556  \n",
       "53997  1.328066  820.136775  0.0  1.0  114.246127  \n",
       "53998  1.075421  820.136775  0.0  1.0  117.898451  \n",
       "53999  1.086612  820.136775  0.0  0.0  118.464716  \n",
       "\n",
       "[54000 rows x 17 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_training.drop([5,11], inplace = True, axis=1)\n",
    "#data.columns\n",
    "#data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>828.235294</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>321.428571</td>\n",
       "      <td>625.911006</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>198.113469</td>\n",
       "      <td>54.142628</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>0.279273</td>\n",
       "      <td>100.083403</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.540594</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.104991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>911.764706</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>247.070921</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>8.181818</td>\n",
       "      <td>198.600020</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>56.802571</td>\n",
       "      <td>4.706554</td>\n",
       "      <td>15.012510</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.684988</td>\n",
       "      <td>1.614613</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>146.654045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>962.352941</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>615.825381</td>\n",
       "      <td>8.181818</td>\n",
       "      <td>197.267767</td>\n",
       "      <td>54.777565</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>0.453212</td>\n",
       "      <td>210.175146</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.044599</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.249570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>892.941177</td>\n",
       "      <td>0.893821</td>\n",
       "      <td>247.070921</td>\n",
       "      <td>638.076431</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>197.355744</td>\n",
       "      <td>4.363431</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>5.424166</td>\n",
       "      <td>116.998948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.509678</td>\n",
       "      <td>1.145729</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.862306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>914.117647</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>248.287889</td>\n",
       "      <td>626.514988</td>\n",
       "      <td>5.181818</td>\n",
       "      <td>198.790477</td>\n",
       "      <td>85.938202</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>2.877702</td>\n",
       "      <td>210.175146</td>\n",
       "      <td>1.830956</td>\n",
       "      <td>11.408517</td>\n",
       "      <td>1.558341</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.268503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53995</td>\n",
       "      <td>961.176471</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>247.070921</td>\n",
       "      <td>617.241675</td>\n",
       "      <td>9.878793</td>\n",
       "      <td>197.148867</td>\n",
       "      <td>32.832680</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>0.614678</td>\n",
       "      <td>116.998948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.998794</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107.493976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53996</td>\n",
       "      <td>823.529412</td>\n",
       "      <td>0.493867</td>\n",
       "      <td>247.070921</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>198.066831</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>56.928321</td>\n",
       "      <td>5.424166</td>\n",
       "      <td>116.998948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.509678</td>\n",
       "      <td>0.970848</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>170.817556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53997</td>\n",
       "      <td>931.764706</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>198.660971</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>49.793908</td>\n",
       "      <td>0.246010</td>\n",
       "      <td>15.012510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>1.328066</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>114.246127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53998</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>197.223243</td>\n",
       "      <td>0.924021</td>\n",
       "      <td>58.331180</td>\n",
       "      <td>0.503800</td>\n",
       "      <td>116.998948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.266667</td>\n",
       "      <td>1.075421</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>117.898451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53999</td>\n",
       "      <td>847.058823</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>622.264355</td>\n",
       "      <td>2.727273</td>\n",
       "      <td>197.742633</td>\n",
       "      <td>0.256672</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>1.561296</td>\n",
       "      <td>40.033361</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>1.086612</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>118.464716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54000 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1           2           3          4           6   \\\n",
       "0       828.235294  0.138889  321.428571  625.911006   1.818182  198.113469   \n",
       "1       911.764706  0.027778  247.070921  611.574748   8.181818  198.600020   \n",
       "2       962.352941  0.833333   35.714286  615.825381   8.181818  197.267767   \n",
       "3       892.941177  0.893821  247.070921  638.076431   9.090909  197.355744   \n",
       "4       914.117647  0.083333  248.287889  626.514988   5.181818  198.790477   \n",
       "...            ...       ...         ...         ...        ...         ...   \n",
       "53995   961.176471  0.055556  247.070921  617.241675   9.878793  197.148867   \n",
       "53996   823.529412  0.493867  247.070921  611.574748  50.000000  198.066831   \n",
       "53997   931.764706  0.027778   35.714286  611.574748   2.272727  198.660971   \n",
       "53998  1000.000000  0.033333   35.714286  611.574748   9.090909  197.223243   \n",
       "53999   847.058823  0.138889   35.714286  622.264355   2.727273  197.742633   \n",
       "\n",
       "              7          8         9           10        12         13  \\\n",
       "0      54.142628  58.632548  0.279273  100.083403  1.000000   1.000000   \n",
       "1      22.086661  56.802571  4.706554   15.012510  1.000000  12.684988   \n",
       "2      54.777565  58.632548  0.453212  210.175146  1.000000  10.000000   \n",
       "3       4.363431  58.632548  5.424166  116.998948  1.000000  12.509678   \n",
       "4      85.938202  58.632548  2.877702  210.175146  1.830956  11.408517   \n",
       "...          ...        ...       ...         ...       ...        ...   \n",
       "53995  32.832680  58.632548  0.614678  116.998948  1.000000   0.333333   \n",
       "53996  22.086661  56.928321  5.424166  116.998948  1.000000  12.509678   \n",
       "53997  22.086661  49.793908  0.246010   15.012510  0.000000   5.600000   \n",
       "53998   0.924021  58.331180  0.503800  116.998948  1.000000   7.266667   \n",
       "53999   0.256672  58.632548  1.561296   40.033361  2.000000   5.333333   \n",
       "\n",
       "             14          15   16   17          18  \n",
       "0      1.540594  820.136775  1.0  0.0  100.104991  \n",
       "1      1.614613  820.136775  0.0  1.0  146.654045  \n",
       "2      1.044599  820.136775  0.0  0.0   98.249570  \n",
       "3      1.145729  820.136775  1.0  0.0  140.862306  \n",
       "4      1.558341  820.136775  1.0  0.0  101.268503  \n",
       "...         ...         ...  ...  ...         ...  \n",
       "53995  0.998794  820.136775  0.0  0.0  107.493976  \n",
       "53996  0.970848  820.136775  0.0  1.0  170.817556  \n",
       "53997  1.328066  820.136775  0.0  1.0  114.246127  \n",
       "53998  1.075421  820.136775  0.0  1.0  117.898451  \n",
       "53999  1.086612  820.136775  0.0  0.0  118.464716  \n",
       "\n",
       "[54000 rows x 17 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_training[\"source\"] = data1[\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>828.235294</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>321.428571</td>\n",
       "      <td>625.911006</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>198.113469</td>\n",
       "      <td>54.142628</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>0.279273</td>\n",
       "      <td>100.083403</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.540594</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.104991</td>\n",
       "      <td>train_df2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>911.764706</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>247.070921</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>8.181818</td>\n",
       "      <td>198.600020</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>56.802571</td>\n",
       "      <td>4.706554</td>\n",
       "      <td>15.012510</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.684988</td>\n",
       "      <td>1.614613</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>146.654045</td>\n",
       "      <td>train_df2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>962.352941</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>615.825381</td>\n",
       "      <td>8.181818</td>\n",
       "      <td>197.267767</td>\n",
       "      <td>54.777565</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>0.453212</td>\n",
       "      <td>210.175146</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.044599</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.249570</td>\n",
       "      <td>train_df2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>892.941177</td>\n",
       "      <td>0.893821</td>\n",
       "      <td>247.070921</td>\n",
       "      <td>638.076431</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>197.355744</td>\n",
       "      <td>4.363431</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>5.424166</td>\n",
       "      <td>116.998948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.509678</td>\n",
       "      <td>1.145729</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.862306</td>\n",
       "      <td>train_df2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>914.117647</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>248.287889</td>\n",
       "      <td>626.514988</td>\n",
       "      <td>5.181818</td>\n",
       "      <td>198.790477</td>\n",
       "      <td>85.938202</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>2.877702</td>\n",
       "      <td>210.175146</td>\n",
       "      <td>1.830956</td>\n",
       "      <td>11.408517</td>\n",
       "      <td>1.558341</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.268503</td>\n",
       "      <td>train_df2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43998</td>\n",
       "      <td>938.823529</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>247.070921</td>\n",
       "      <td>615.121611</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>198.119869</td>\n",
       "      <td>0.799962</td>\n",
       "      <td>50.541044</td>\n",
       "      <td>0.183641</td>\n",
       "      <td>40.033361</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.508269</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125.161790</td>\n",
       "      <td>leader_df</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43999</td>\n",
       "      <td>995.294118</td>\n",
       "      <td>0.421029</td>\n",
       "      <td>247.853586</td>\n",
       "      <td>612.884251</td>\n",
       "      <td>3.636364</td>\n",
       "      <td>197.078829</td>\n",
       "      <td>46.962240</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>1.509322</td>\n",
       "      <td>300.250208</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>1.034153</td>\n",
       "      <td>996.330000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.845095</td>\n",
       "      <td>leader_df</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>984.705882</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>3.454545</td>\n",
       "      <td>196.911295</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>47.899686</td>\n",
       "      <td>0.126123</td>\n",
       "      <td>117.685673</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>0.959315</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.032702</td>\n",
       "      <td>evaluation_df</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44001</td>\n",
       "      <td>815.294118</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>392.857143</td>\n",
       "      <td>638.878239</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>199.830468</td>\n",
       "      <td>54.142628</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>1.192628</td>\n",
       "      <td>50.041701</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.949250</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.507745</td>\n",
       "      <td>evaluation_df</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44002</td>\n",
       "      <td>872.941177</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>714.285714</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>8.181818</td>\n",
       "      <td>197.352191</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>56.758169</td>\n",
       "      <td>6.295772</td>\n",
       "      <td>15.012510</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.733333</td>\n",
       "      <td>1.122608</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>evaluation_df</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44003 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1           2           3          4           6  \\\n",
       "0      828.235294  0.138889  321.428571  625.911006   1.818182  198.113469   \n",
       "1      911.764706  0.027778  247.070921  611.574748   8.181818  198.600020   \n",
       "2      962.352941  0.833333   35.714286  615.825381   8.181818  197.267767   \n",
       "3      892.941177  0.893821  247.070921  638.076431   9.090909  197.355744   \n",
       "4      914.117647  0.083333  248.287889  626.514988   5.181818  198.790477   \n",
       "...           ...       ...         ...         ...        ...         ...   \n",
       "43998  938.823529  0.166667  247.070921  615.121611  11.363636  198.119869   \n",
       "43999  995.294118  0.421029  247.853586  612.884251   3.636364  197.078829   \n",
       "44000  984.705882  0.005556   35.714286  611.574748   3.454545  196.911295   \n",
       "44001  815.294118  0.333333  392.857143  638.878239   9.090909  199.830468   \n",
       "44002  872.941177  0.138889  714.285714  611.574748   8.181818  197.352191   \n",
       "\n",
       "               7          8         9          10        12         13  \\\n",
       "0      54.142628  58.632548  0.279273  100.083403  1.000000   1.000000   \n",
       "1      22.086661  56.802571  4.706554   15.012510  1.000000  12.684988   \n",
       "2      54.777565  58.632548  0.453212  210.175146  1.000000  10.000000   \n",
       "3       4.363431  58.632548  5.424166  116.998948  1.000000  12.509678   \n",
       "4      85.938202  58.632548  2.877702  210.175146  1.830956  11.408517   \n",
       "...          ...        ...       ...         ...       ...        ...   \n",
       "43998   0.799962  50.541044  0.183641   40.033361  3.000000  10.000000   \n",
       "43999  46.962240  58.632548  1.509322  300.250208  2.000000  16.666667   \n",
       "44000  22.086661  47.899686  0.126123  117.685673  1.000000   6.666667   \n",
       "44001  54.142628  58.632548  1.192628   50.041701  1.000000  10.000000   \n",
       "44002  22.086661  56.758169  6.295772   15.012510  1.000000   6.733333   \n",
       "\n",
       "             14           15   16   17          18         source  \n",
       "0      1.540594   820.136775  1.0  0.0  100.104991      train_df2  \n",
       "1      1.614613   820.136775  0.0  1.0  146.654045      train_df2  \n",
       "2      1.044599   820.136775  0.0  0.0   98.249570      train_df2  \n",
       "3      1.145729   820.136775  1.0  0.0  140.862306      train_df2  \n",
       "4      1.558341   820.136775  1.0  0.0  101.268503      train_df2  \n",
       "...         ...          ...  ...  ...         ...            ...  \n",
       "43998  1.508269   820.136775  1.0  1.0  125.161790      leader_df  \n",
       "43999  1.034153   996.330000  1.0  0.0  102.845095      leader_df  \n",
       "44000  0.959315   820.136775  0.0  1.0   99.032702  evaluation_df  \n",
       "44001  1.949250   820.136775  1.0  0.0   82.507745  evaluation_df  \n",
       "44002  1.122608  1000.000000  0.0  1.0    0.000000  evaluation_df  \n",
       "\n",
       "[44003 rows x 18 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_training.head(44003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seg = imputed_training.iloc[:34000,:]\n",
    "leaderboard_seg = imputed_training.iloc[34000:44000,:]\n",
    "evaluation_seg = imputed_training.iloc[44000:,:]\n",
    "#imputed_training.drop([\"source\"],inplace = True , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>828.235294</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>321.428571</td>\n",
       "      <td>625.911006</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>198.113469</td>\n",
       "      <td>54.142628</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>0.279273</td>\n",
       "      <td>100.083403</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.540594</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.104991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>911.764706</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>247.070921</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>8.181818</td>\n",
       "      <td>198.600020</td>\n",
       "      <td>22.086661</td>\n",
       "      <td>56.802571</td>\n",
       "      <td>4.706554</td>\n",
       "      <td>15.012510</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.684988</td>\n",
       "      <td>1.614613</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>146.654045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>962.352941</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>615.825381</td>\n",
       "      <td>8.181818</td>\n",
       "      <td>197.267767</td>\n",
       "      <td>54.777565</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>0.453212</td>\n",
       "      <td>210.175146</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.044599</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.249570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>892.941177</td>\n",
       "      <td>0.893821</td>\n",
       "      <td>247.070921</td>\n",
       "      <td>638.076431</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>197.355744</td>\n",
       "      <td>4.363431</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>5.424166</td>\n",
       "      <td>116.998948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.509678</td>\n",
       "      <td>1.145729</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.862306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>914.117647</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>248.287889</td>\n",
       "      <td>626.514988</td>\n",
       "      <td>5.181818</td>\n",
       "      <td>198.790477</td>\n",
       "      <td>85.938202</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>2.877702</td>\n",
       "      <td>210.175146</td>\n",
       "      <td>1.830956</td>\n",
       "      <td>11.408517</td>\n",
       "      <td>1.558341</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.268503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33995</td>\n",
       "      <td>996.470588</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>247.070921</td>\n",
       "      <td>611.574748</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>199.590730</td>\n",
       "      <td>1.907932</td>\n",
       "      <td>57.227776</td>\n",
       "      <td>1.837104</td>\n",
       "      <td>250.208507</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>1.913514</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>155.080895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33996</td>\n",
       "      <td>885.882353</td>\n",
       "      <td>1.164197</td>\n",
       "      <td>247.070921</td>\n",
       "      <td>617.448254</td>\n",
       "      <td>22.727273</td>\n",
       "      <td>197.743579</td>\n",
       "      <td>4.902443</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>5.424166</td>\n",
       "      <td>5.004170</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.509678</td>\n",
       "      <td>1.014169</td>\n",
       "      <td>820.136775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.972461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33997</td>\n",
       "      <td>976.470588</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>621.613105</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>197.437342</td>\n",
       "      <td>58.431835</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>1.172531</td>\n",
       "      <td>130.108424</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>1.228365</td>\n",
       "      <td>367.020000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.767642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33998</td>\n",
       "      <td>785.882353</td>\n",
       "      <td>0.177233</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>682.762351</td>\n",
       "      <td>6.923859</td>\n",
       "      <td>198.254389</td>\n",
       "      <td>9.359987</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>8.072586</td>\n",
       "      <td>18.348624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.866667</td>\n",
       "      <td>0.976599</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.471601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33999</td>\n",
       "      <td>917.647059</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>428.571429</td>\n",
       "      <td>650.416923</td>\n",
       "      <td>11.818182</td>\n",
       "      <td>201.389811</td>\n",
       "      <td>54.142628</td>\n",
       "      <td>58.632548</td>\n",
       "      <td>29.354858</td>\n",
       "      <td>150.125104</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.319554</td>\n",
       "      <td>942.850000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.588640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34000 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1            2           3          4           6   \\\n",
       "0      828.235294  0.138889   321.428571  625.911006   1.818182  198.113469   \n",
       "1      911.764706  0.027778   247.070921  611.574748   8.181818  198.600020   \n",
       "2      962.352941  0.833333    35.714286  615.825381   8.181818  197.267767   \n",
       "3      892.941177  0.893821   247.070921  638.076431   9.090909  197.355744   \n",
       "4      914.117647  0.083333   248.287889  626.514988   5.181818  198.790477   \n",
       "...           ...       ...          ...         ...        ...         ...   \n",
       "33995  996.470588  0.833333   247.070921  611.574748   9.090909  199.590730   \n",
       "33996  885.882353  1.164197   247.070921  617.448254  22.727273  197.743579   \n",
       "33997  976.470588  0.166667    35.714286  621.613105  18.181818  197.437342   \n",
       "33998  785.882353  0.177233  1000.000000  682.762351   6.923859  198.254389   \n",
       "33999  917.647059  0.138889   428.571429  650.416923  11.818182  201.389811   \n",
       "\n",
       "              7          8          9           10        12         13  \\\n",
       "0      54.142628  58.632548   0.279273  100.083403  1.000000   1.000000   \n",
       "1      22.086661  56.802571   4.706554   15.012510  1.000000  12.684988   \n",
       "2      54.777565  58.632548   0.453212  210.175146  1.000000  10.000000   \n",
       "3       4.363431  58.632548   5.424166  116.998948  1.000000  12.509678   \n",
       "4      85.938202  58.632548   2.877702  210.175146  1.830956  11.408517   \n",
       "...          ...        ...        ...         ...       ...        ...   \n",
       "33995   1.907932  57.227776   1.837104  250.208507  1.000000  13.333333   \n",
       "33996   4.902443  58.632548   5.424166    5.004170  1.000000  12.509678   \n",
       "33997  58.431835  58.632548   1.172531  130.108424  1.000000  12.666667   \n",
       "33998   9.359987  58.632548   8.072586   18.348624  1.000000  11.866667   \n",
       "33999  54.142628  58.632548  29.354858  150.125104  1.000000  10.000000   \n",
       "\n",
       "             14           15   16   17          18  \n",
       "0      1.540594   820.136775  1.0  0.0  100.104991  \n",
       "1      1.614613   820.136775  0.0  1.0  146.654045  \n",
       "2      1.044599   820.136775  0.0  0.0   98.249570  \n",
       "3      1.145729   820.136775  1.0  0.0  140.862306  \n",
       "4      1.558341   820.136775  1.0  0.0  101.268503  \n",
       "...         ...          ...  ...  ...         ...  \n",
       "33995  1.913514   820.136775  1.0  1.0  155.080895  \n",
       "33996  1.014169   820.136775  0.0  0.0  162.972461  \n",
       "33997  1.228365   367.020000  0.0  0.0  108.767642  \n",
       "33998  0.976599  1000.000000  0.0  0.0  101.471601  \n",
       "33999  2.319554   942.850000  1.0  0.0   52.588640  \n",
       "\n",
       "[34000 rows x 17 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seg.to_csv(r'C:\\Users\\ANIMESH TIWARI\\Desktop\\train_seg.csv')\n",
    "leaderboard_seg.to_csv(r'C:\\Users\\ANIMESH TIWARI\\Desktop\\leaderboard_seg.csv')\n",
    "evaluation_seg.to_csv(r'C:\\Users\\ANIMESH TIWARI\\Desktop\\evaluation_seg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from numpy import loadtxt\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()    \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_seg = pd.DataFrame(train_seg)\n",
    "#train_seg.drop([\"source\"], inplace = True , axis = 1)\n",
    "train_seg.loc[:, :] = scaler.fit_transform(train_seg.loc[:, :])\n",
    "\n",
    "# split into input (X) and output (y) variables\n",
    "train_seg = pd.DataFrame(train_seg)\n",
    "X = train_seg.iloc[:,:]\n",
    "y = train_df.iloc[:,19]\n",
    "y = lb.fit_transform(y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28900,)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: D:\\Anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - lightgbm\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2019.9.11  |       hecc5488_0         181 KB  conda-forge\n",
      "    conda-4.7.12               |           py37_0         3.0 MB  conda-forge\n",
      "    joblib-0.14.0              |             py_0         197 KB  conda-forge\n",
      "    libblas-3.8.0              |           12_mkl         3.5 MB  conda-forge\n",
      "    libcblas-3.8.0             |           12_mkl         3.5 MB  conda-forge\n",
      "    lightgbm-2.3.0             |   py37h6538335_0         553 KB  conda-forge\n",
      "    openssl-1.1.1c             |       hfa6e2cd_0         4.7 MB  conda-forge\n",
      "    scikit-learn-0.21.3        |   py37h7208079_0         5.7 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        21.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  joblib             conda-forge/noarch::joblib-0.14.0-py_0\n",
      "  libblas            conda-forge/win-64::libblas-3.8.0-12_mkl\n",
      "  libcblas           conda-forge/win-64::libcblas-3.8.0-12_mkl\n",
      "  lightgbm           conda-forge/win-64::lightgbm-2.3.0-py37h6538335_0\n",
      "  scikit-learn       conda-forge/win-64::scikit-learn-0.21.3-py37h7208079_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2019.8.28-0 --> conda-forge::ca-certificates-2019.9.11-hecc5488_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  conda                                           pkgs/main --> conda-forge\n",
      "  openssl              pkgs/main::openssl-1.1.1d-he774522_2 --> conda-forge::openssl-1.1.1c-hfa6e2cd_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "conda-4.7.12         | 3.0 MB    |            |   0% \n",
      "conda-4.7.12         | 3.0 MB    |            |   1% \n",
      "conda-4.7.12         | 3.0 MB    | 2          |   3% \n",
      "conda-4.7.12         | 3.0 MB    | 5          |   6% \n",
      "conda-4.7.12         | 3.0 MB    | #2         |  12% \n",
      "conda-4.7.12         | 3.0 MB    | ##         |  21% \n",
      "conda-4.7.12         | 3.0 MB    | ##5        |  26% \n",
      "conda-4.7.12         | 3.0 MB    | ###5       |  36% \n",
      "conda-4.7.12         | 3.0 MB    | ####5      |  46% \n",
      "conda-4.7.12         | 3.0 MB    | #####5     |  56% \n",
      "conda-4.7.12         | 3.0 MB    | ######5    |  65% \n",
      "conda-4.7.12         | 3.0 MB    | #######4   |  75% \n",
      "conda-4.7.12         | 3.0 MB    | ########4  |  84% \n",
      "conda-4.7.12         | 3.0 MB    | #########4 |  94% \n",
      "conda-4.7.12         | 3.0 MB    | ########## | 100% \n",
      "\n",
      "openssl-1.1.1c       | 4.7 MB    |            |   0% \n",
      "openssl-1.1.1c       | 4.7 MB    |            |   0% \n",
      "openssl-1.1.1c       | 4.7 MB    | 6          |   6% \n",
      "openssl-1.1.1c       | 4.7 MB    | #2         |  12% \n",
      "openssl-1.1.1c       | 4.7 MB    | #7         |  18% \n",
      "openssl-1.1.1c       | 4.7 MB    | ##1        |  22% \n",
      "openssl-1.1.1c       | 4.7 MB    | ##7        |  27% \n",
      "openssl-1.1.1c       | 4.7 MB    | ###3       |  33% \n",
      "openssl-1.1.1c       | 4.7 MB    | ###8       |  39% \n",
      "openssl-1.1.1c       | 4.7 MB    | ####4      |  45% \n",
      "openssl-1.1.1c       | 4.7 MB    | #####      |  51% \n",
      "openssl-1.1.1c       | 4.7 MB    | #####6     |  56% \n",
      "openssl-1.1.1c       | 4.7 MB    | ######1    |  61% \n",
      "openssl-1.1.1c       | 4.7 MB    | ######5    |  66% \n",
      "openssl-1.1.1c       | 4.7 MB    | ######9    |  69% \n",
      "openssl-1.1.1c       | 4.7 MB    | #######3   |  73% \n",
      "openssl-1.1.1c       | 4.7 MB    | #######6   |  76% \n",
      "openssl-1.1.1c       | 4.7 MB    | ########   |  81% \n",
      "openssl-1.1.1c       | 4.7 MB    | ########5  |  86% \n",
      "openssl-1.1.1c       | 4.7 MB    | #########1 |  92% \n",
      "openssl-1.1.1c       | 4.7 MB    | #########7 |  98% \n",
      "openssl-1.1.1c       | 4.7 MB    | ########## | 100% \n",
      "\n",
      "ca-certificates-2019 | 181 KB    |            |   0% \n",
      "ca-certificates-2019 | 181 KB    | 8          |   9% \n",
      "ca-certificates-2019 | 181 KB    | ###5       |  35% \n",
      "ca-certificates-2019 | 181 KB    | #######9   |  80% \n",
      "ca-certificates-2019 | 181 KB    | ########## | 100% \n",
      "\n",
      "joblib-0.14.0        | 197 KB    |            |   0% \n",
      "joblib-0.14.0        | 197 KB    | 8          |   8% \n",
      "joblib-0.14.0        | 197 KB    | #######3   |  73% \n",
      "joblib-0.14.0        | 197 KB    | ########## | 100% \n",
      "\n",
      "lightgbm-2.3.0       | 553 KB    |            |   0% \n",
      "lightgbm-2.3.0       | 553 KB    | 2          |   3% \n",
      "lightgbm-2.3.0       | 553 KB    | #4         |  14% \n",
      "lightgbm-2.3.0       | 553 KB    | ######     |  61% \n",
      "lightgbm-2.3.0       | 553 KB    | #######8   |  78% \n",
      "lightgbm-2.3.0       | 553 KB    | ########## | 100% \n",
      "\n",
      "libblas-3.8.0        | 3.5 MB    |            |   0% \n",
      "libblas-3.8.0        | 3.5 MB    |            |   0% \n",
      "libblas-3.8.0        | 3.5 MB    | 4          |   5% \n",
      "libblas-3.8.0        | 3.5 MB    | #          |  11% \n",
      "libblas-3.8.0        | 3.5 MB    | #5         |  16% \n",
      "libblas-3.8.0        | 3.5 MB    | ##1        |  22% \n",
      "libblas-3.8.0        | 3.5 MB    | ###1       |  31% \n",
      "libblas-3.8.0        | 3.5 MB    | ###8       |  39% \n",
      "libblas-3.8.0        | 3.5 MB    | ####6      |  47% \n",
      "libblas-3.8.0        | 3.5 MB    | #####4     |  54% \n",
      "libblas-3.8.0        | 3.5 MB    | ######2    |  63% \n",
      "libblas-3.8.0        | 3.5 MB    | #######    |  70% \n",
      "libblas-3.8.0        | 3.5 MB    | #######8   |  78% \n",
      "libblas-3.8.0        | 3.5 MB    | ########6  |  86% \n",
      "libblas-3.8.0        | 3.5 MB    | #########3 |  94% \n",
      "libblas-3.8.0        | 3.5 MB    | ########## | 100% \n",
      "\n",
      "libcblas-3.8.0       | 3.5 MB    |            |   0% \n",
      "libcblas-3.8.0       | 3.5 MB    |            |   0% \n",
      "libcblas-3.8.0       | 3.5 MB    | 7          |   8% \n",
      "libcblas-3.8.0       | 3.5 MB    | #3         |  14% \n",
      "libcblas-3.8.0       | 3.5 MB    | ##2        |  23% \n",
      "libcblas-3.8.0       | 3.5 MB    | ##7        |  28% \n",
      "libcblas-3.8.0       | 3.5 MB    | ###3       |  33% \n",
      "libcblas-3.8.0       | 3.5 MB    | ###9       |  39% \n",
      "libcblas-3.8.0       | 3.5 MB    | ####9      |  49% \n",
      "libcblas-3.8.0       | 3.5 MB    | #####7     |  58% \n",
      "libcblas-3.8.0       | 3.5 MB    | ######6    |  66% \n",
      "libcblas-3.8.0       | 3.5 MB    | #######4   |  74% \n",
      "libcblas-3.8.0       | 3.5 MB    | ########2  |  82% \n",
      "libcblas-3.8.0       | 3.5 MB    | #########  |  90% \n",
      "libcblas-3.8.0       | 3.5 MB    | #########8 |  98% \n",
      "libcblas-3.8.0       | 3.5 MB    | ########## | 100% \n",
      "\n",
      "scikit-learn-0.21.3  | 5.7 MB    |            |   0% \n",
      "scikit-learn-0.21.3  | 5.7 MB    |            |   1% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | 6          |   6% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | #          |  11% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | #5         |  16% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | ##         |  21% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | ##5        |  26% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | ###        |  30% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | ###5       |  35% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | ####       |  40% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | ####5      |  45% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | ####9      |  50% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | #####4     |  54% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | #####8     |  59% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | ######3    |  63% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | ######8    |  69% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | #######3   |  74% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | #######8   |  79% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | ########3  |  84% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | ########8  |  88% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | #########3 |  94% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | #########8 |  99% \n",
      "scikit-learn-0.21.3  | 5.7 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "\n",
      "LightGBM: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.52      0.05      0.09       590\n",
      "     class_1       0.54      0.33      0.41      1970\n",
      "     class_2       0.57      0.83      0.68      2880\n",
      "\n",
      "    accuracy                           0.57      5440\n",
      "   macro avg       0.54      0.40      0.39      5440\n",
      "weighted avg       0.56      0.57      0.52      5440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from lightgbm import LGBMClassifier\n",
    "dataset = datasets.load_wine()\n",
    "x_seg = dataset.data; y2 = dataset.target\n",
    "model = LGBMClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "print(); print(model)\n",
    "\n",
    "    # make predictions\n",
    "expected_y  = y_test\n",
    "predicted_y = model.predict(x_test)\n",
    "    \n",
    "    # summarize the fit of the model\n",
    "print(); print('LightGBM: ')\n",
    "print(); print(metrics.classification_report(expected_y, predicted_y, target_names=dataset.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, ..., 2, 2, 1])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6 50\n",
      "\n",
      "LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.3, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=50, n_jobs=-1, num_leaves=50, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "\n",
      "LightGBM: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.46      0.06      0.11       590\n",
      "     class_1       0.54      0.34      0.42      1970\n",
      "     class_2       0.57      0.82      0.68      2880\n",
      "\n",
      "    accuracy                           0.57      5440\n",
      "   macro avg       0.52      0.41      0.40      5440\n",
      "weighted avg       0.55      0.57      0.52      5440\n",
      "\n",
      "0.6 75\n",
      "\n",
      "LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.3, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=50, n_jobs=-1, num_leaves=50, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "\n",
      "LightGBM: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.46      0.06      0.11       590\n",
      "     class_1       0.54      0.34      0.42      1970\n",
      "     class_2       0.57      0.82      0.68      2880\n",
      "\n",
      "    accuracy                           0.57      5440\n",
      "   macro avg       0.52      0.41      0.40      5440\n",
      "weighted avg       0.55      0.57      0.52      5440\n",
      "\n",
      "0.6 100\n",
      "\n",
      "LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.3, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=50, n_jobs=-1, num_leaves=50, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "\n",
      "LightGBM: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.46      0.06      0.11       590\n",
      "     class_1       0.54      0.34      0.42      1970\n",
      "     class_2       0.57      0.82      0.68      2880\n",
      "\n",
      "    accuracy                           0.57      5440\n",
      "   macro avg       0.52      0.41      0.40      5440\n",
      "weighted avg       0.55      0.57      0.52      5440\n",
      "\n",
      "0.6 125\n",
      "\n",
      "LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.3, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=50, n_jobs=-1, num_leaves=50, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "\n",
      "LightGBM: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.46      0.06      0.11       590\n",
      "     class_1       0.54      0.34      0.42      1970\n",
      "     class_2       0.57      0.82      0.68      2880\n",
      "\n",
      "    accuracy                           0.57      5440\n",
      "   macro avg       0.52      0.41      0.40      5440\n",
      "weighted avg       0.55      0.57      0.52      5440\n",
      "\n",
      "0.6 150\n",
      "\n",
      "LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.3, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=50, n_jobs=-1, num_leaves=50, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "\n",
      "LightGBM: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.46      0.06      0.11       590\n",
      "     class_1       0.54      0.34      0.42      1970\n",
      "     class_2       0.57      0.82      0.68      2880\n",
      "\n",
      "    accuracy                           0.57      5440\n",
      "   macro avg       0.52      0.41      0.40      5440\n",
      "weighted avg       0.55      0.57      0.52      5440\n",
      "\n",
      "0.6 175\n",
      "\n",
      "LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.3, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=50, n_jobs=-1, num_leaves=50, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "\n",
      "LightGBM: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.46      0.06      0.11       590\n",
      "     class_1       0.54      0.34      0.42      1970\n",
      "     class_2       0.57      0.82      0.68      2880\n",
      "\n",
      "    accuracy                           0.57      5440\n",
      "   macro avg       0.52      0.41      0.40      5440\n",
      "weighted avg       0.55      0.57      0.52      5440\n",
      "\n",
      "0.6 200\n",
      "\n",
      "LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.3, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=50, n_jobs=-1, num_leaves=50, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "\n",
      "LightGBM: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.46      0.06      0.11       590\n",
      "     class_1       0.54      0.34      0.42      1970\n",
      "     class_2       0.57      0.82      0.68      2880\n",
      "\n",
      "    accuracy                           0.57      5440\n",
      "   macro avg       0.52      0.41      0.40      5440\n",
      "weighted avg       0.55      0.57      0.52      5440\n",
      "\n",
      "0.6 225\n",
      "\n",
      "LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.3, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=50, n_jobs=-1, num_leaves=50, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "\n",
      "LightGBM: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.46      0.06      0.11       590\n",
      "     class_1       0.54      0.34      0.42      1970\n",
      "     class_2       0.57      0.82      0.68      2880\n",
      "\n",
      "    accuracy                           0.57      5440\n",
      "   macro avg       0.52      0.41      0.40      5440\n",
      "weighted avg       0.55      0.57      0.52      5440\n",
      "\n",
      "0.6 250\n",
      "\n",
      "LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.3, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=50, n_jobs=-1, num_leaves=50, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "\n",
      "LightGBM: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.46      0.06      0.11       590\n",
      "     class_1       0.54      0.34      0.42      1970\n",
      "     class_2       0.57      0.82      0.68      2880\n",
      "\n",
      "    accuracy                           0.57      5440\n",
      "   macro avg       0.52      0.41      0.40      5440\n",
      "weighted avg       0.55      0.57      0.52      5440\n",
      "\n",
      "0.6 275\n",
      "\n",
      "LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.3, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=50, n_jobs=-1, num_leaves=50, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "\n",
      "LightGBM: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.46      0.06      0.11       590\n",
      "     class_1       0.54      0.34      0.42      1970\n",
      "     class_2       0.57      0.82      0.68      2880\n",
      "\n",
      "    accuracy                           0.57      5440\n",
      "   macro avg       0.52      0.41      0.40      5440\n",
      "weighted avg       0.55      0.57      0.52      5440\n",
      "\n",
      "0.6 300\n",
      "\n",
      "LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.3, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=50, n_jobs=-1, num_leaves=50, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "\n",
      "LightGBM: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.46      0.06      0.11       590\n",
      "     class_1       0.54      0.34      0.42      1970\n",
      "     class_2       0.57      0.82      0.68      2880\n",
      "\n",
      "    accuracy                           0.57      5440\n",
      "   macro avg       0.52      0.41      0.40      5440\n",
      "weighted avg       0.55      0.57      0.52      5440\n",
      "\n",
      "0.6 325\n",
      "\n",
      "LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.3, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=50, n_jobs=-1, num_leaves=50, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "\n",
      "LightGBM: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.46      0.06      0.11       590\n",
      "     class_1       0.54      0.34      0.42      1970\n",
      "     class_2       0.57      0.82      0.68      2880\n",
      "\n",
      "    accuracy                           0.57      5440\n",
      "   macro avg       0.52      0.41      0.40      5440\n",
      "weighted avg       0.55      0.57      0.52      5440\n",
      "\n",
      "0.6 350\n",
      "\n",
      "LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.3, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=50, n_jobs=-1, num_leaves=50, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "\n",
      "LightGBM: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.46      0.06      0.11       590\n",
      "     class_1       0.54      0.34      0.42      1970\n",
      "     class_2       0.57      0.82      0.68      2880\n",
      "\n",
      "    accuracy                           0.57      5440\n",
      "   macro avg       0.52      0.41      0.40      5440\n",
      "weighted avg       0.55      0.57      0.52      5440\n",
      "\n",
      "0.6 375\n",
      "\n",
      "LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.3, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=50, n_jobs=-1, num_leaves=50, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "\n",
      "LightGBM: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.46      0.06      0.11       590\n",
      "     class_1       0.54      0.34      0.42      1970\n",
      "     class_2       0.57      0.82      0.68      2880\n",
      "\n",
      "    accuracy                           0.57      5440\n",
      "   macro avg       0.52      0.41      0.40      5440\n",
      "weighted avg       0.55      0.57      0.52      5440\n",
      "\n",
      "0.6 400\n",
      "\n",
      "LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.3, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=50, n_jobs=-1, num_leaves=50, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "\n",
      "LightGBM: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.46      0.06      0.11       590\n",
      "     class_1       0.54      0.34      0.42      1970\n",
      "     class_2       0.57      0.82      0.68      2880\n",
      "\n",
      "    accuracy                           0.57      5440\n",
      "   macro avg       0.52      0.41      0.40      5440\n",
      "weighted avg       0.55      0.57      0.52      5440\n",
      "\n",
      "0.6 425\n",
      "\n",
      "LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.3, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=50, n_jobs=-1, num_leaves=50, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "\n",
      "LightGBM: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.46      0.06      0.11       590\n",
      "     class_1       0.54      0.34      0.42      1970\n",
      "     class_2       0.57      0.82      0.68      2880\n",
      "\n",
      "    accuracy                           0.57      5440\n",
      "   macro avg       0.52      0.41      0.40      5440\n",
      "weighted avg       0.55      0.57      0.52      5440\n",
      "\n",
      "0.6 450\n",
      "\n",
      "LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.3, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=50, n_jobs=-1, num_leaves=50, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "\n",
      "LightGBM: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.46      0.06      0.11       590\n",
      "     class_1       0.54      0.34      0.42      1970\n",
      "     class_2       0.57      0.82      0.68      2880\n",
      "\n",
      "    accuracy                           0.57      5440\n",
      "   macro avg       0.52      0.41      0.40      5440\n",
      "weighted avg       0.55      0.57      0.52      5440\n",
      "\n",
      "0.6 475\n",
      "\n",
      "LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.3, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=50, n_jobs=-1, num_leaves=50, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "\n",
      "LightGBM: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.46      0.06      0.11       590\n",
      "     class_1       0.54      0.34      0.42      1970\n",
      "     class_2       0.57      0.82      0.68      2880\n",
      "\n",
      "    accuracy                           0.57      5440\n",
      "   macro avg       0.52      0.41      0.40      5440\n",
      "weighted avg       0.55      0.57      0.52      5440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for j in range(50, 500, 25):\n",
    "        print(i, j)\n",
    "        model = LGBMClassifier(boosting_type = 'dart', learning_rate=0.3, num_leaves=50, n_estimators=50)\n",
    "        model.fit(x_train, y_train)\n",
    "        print(); print(model)\n",
    "\n",
    "    # make predictions\n",
    "        expected_y  = y_test\n",
    "        predicted_y = model.predict(x_test)\n",
    "    \n",
    "    # summarize the fit of the model\n",
    "        print(); print('LightGBM: ')\n",
    "        print(); print(metrics.classification_report(expected_y, predicted_y, target_names=dataset.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.49279372e-16, 6.70262110e-02, 2.52198588e-01, 6.80775201e-01],\n",
       "       [8.63188405e-16, 1.30552932e-01, 2.65740430e-01, 6.03706638e-01],\n",
       "       [5.50538786e-16, 6.94717257e-01, 1.40843092e-02, 2.91198433e-01],\n",
       "       ...,\n",
       "       [1.01012073e-15, 1.83144980e-01, 2.76931166e-01, 5.39923853e-01],\n",
       "       [9.29103407e-16, 2.36481325e-01, 1.98747492e-01, 5.64771184e-01],\n",
       "       [1.20743469e-15, 1.55136511e-01, 2.39644971e-01, 6.05218518e-01]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "27200/27200 [==============================] - ETA: 6:39 - loss: 1.6531 - accuracy: 0.06 - ETA: 26s - loss: 1.1784 - accuracy: 0.3229 - ETA: 16s - loss: 1.0798 - accuracy: 0.421 - ETA: 14s - loss: 1.0479 - accuracy: 0.433 - ETA: 11s - loss: 1.0163 - accuracy: 0.456 - ETA: 9s - loss: 0.9966 - accuracy: 0.469 - ETA: 8s - loss: 0.9852 - accuracy: 0.48 - ETA: 8s - loss: 0.9751 - accuracy: 0.50 - ETA: 7s - loss: 0.9744 - accuracy: 0.50 - ETA: 7s - loss: 0.9692 - accuracy: 0.51 - ETA: 6s - loss: 0.9726 - accuracy: 0.51 - ETA: 6s - loss: 0.9736 - accuracy: 0.51 - ETA: 6s - loss: 0.9668 - accuracy: 0.51 - ETA: 6s - loss: 0.9644 - accuracy: 0.52 - ETA: 5s - loss: 0.9600 - accuracy: 0.52 - ETA: 5s - loss: 0.9585 - accuracy: 0.52 - ETA: 5s - loss: 0.9570 - accuracy: 0.52 - ETA: 5s - loss: 0.9565 - accuracy: 0.53 - ETA: 5s - loss: 0.9541 - accuracy: 0.53 - ETA: 4s - loss: 0.9523 - accuracy: 0.53 - ETA: 4s - loss: 0.9512 - accuracy: 0.53 - ETA: 4s - loss: 0.9483 - accuracy: 0.53 - ETA: 4s - loss: 0.9493 - accuracy: 0.53 - ETA: 4s - loss: 0.9492 - accuracy: 0.53 - ETA: 4s - loss: 0.9480 - accuracy: 0.53 - ETA: 4s - loss: 0.9503 - accuracy: 0.53 - ETA: 4s - loss: 0.9494 - accuracy: 0.53 - ETA: 4s - loss: 0.9490 - accuracy: 0.53 - ETA: 3s - loss: 0.9493 - accuracy: 0.52 - ETA: 3s - loss: 0.9489 - accuracy: 0.52 - ETA: 3s - loss: 0.9490 - accuracy: 0.52 - ETA: 3s - loss: 0.9495 - accuracy: 0.52 - ETA: 3s - loss: 0.9486 - accuracy: 0.53 - ETA: 3s - loss: 0.9479 - accuracy: 0.53 - ETA: 3s - loss: 0.9462 - accuracy: 0.53 - ETA: 3s - loss: 0.9458 - accuracy: 0.53 - ETA: 3s - loss: 0.9456 - accuracy: 0.53 - ETA: 3s - loss: 0.9442 - accuracy: 0.53 - ETA: 3s - loss: 0.9428 - accuracy: 0.53 - ETA: 3s - loss: 0.9424 - accuracy: 0.53 - ETA: 3s - loss: 0.9412 - accuracy: 0.53 - ETA: 3s - loss: 0.9419 - accuracy: 0.53 - ETA: 3s - loss: 0.9420 - accuracy: 0.53 - ETA: 2s - loss: 0.9416 - accuracy: 0.53 - ETA: 2s - loss: 0.9426 - accuracy: 0.53 - ETA: 2s - loss: 0.9423 - accuracy: 0.53 - ETA: 2s - loss: 0.9421 - accuracy: 0.53 - ETA: 2s - loss: 0.9424 - accuracy: 0.53 - ETA: 2s - loss: 0.9424 - accuracy: 0.53 - ETA: 2s - loss: 0.9415 - accuracy: 0.53 - ETA: 2s - loss: 0.9416 - accuracy: 0.53 - ETA: 2s - loss: 0.9422 - accuracy: 0.53 - ETA: 2s - loss: 0.9420 - accuracy: 0.53 - ETA: 2s - loss: 0.9420 - accuracy: 0.53 - ETA: 2s - loss: 0.9424 - accuracy: 0.53 - ETA: 2s - loss: 0.9422 - accuracy: 0.53 - ETA: 2s - loss: 0.9420 - accuracy: 0.53 - ETA: 1s - loss: 0.9410 - accuracy: 0.53 - ETA: 1s - loss: 0.9407 - accuracy: 0.53 - ETA: 1s - loss: 0.9399 - accuracy: 0.53 - ETA: 1s - loss: 0.9395 - accuracy: 0.53 - ETA: 1s - loss: 0.9398 - accuracy: 0.53 - ETA: 1s - loss: 0.9403 - accuracy: 0.53 - ETA: 1s - loss: 0.9400 - accuracy: 0.53 - ETA: 1s - loss: 0.9395 - accuracy: 0.53 - ETA: 1s - loss: 0.9390 - accuracy: 0.53 - ETA: 1s - loss: 0.9391 - accuracy: 0.53 - ETA: 1s - loss: 0.9386 - accuracy: 0.53 - ETA: 1s - loss: 0.9381 - accuracy: 0.53 - ETA: 1s - loss: 0.9385 - accuracy: 0.53 - ETA: 1s - loss: 0.9388 - accuracy: 0.53 - ETA: 1s - loss: 0.9383 - accuracy: 0.53 - ETA: 1s - loss: 0.9385 - accuracy: 0.53 - ETA: 1s - loss: 0.9391 - accuracy: 0.53 - ETA: 1s - loss: 0.9380 - accuracy: 0.53 - ETA: 0s - loss: 0.9375 - accuracy: 0.53 - ETA: 0s - loss: 0.9376 - accuracy: 0.53 - ETA: 0s - loss: 0.9365 - accuracy: 0.54 - ETA: 0s - loss: 0.9363 - accuracy: 0.54 - ETA: 0s - loss: 0.9363 - accuracy: 0.54 - ETA: 0s - loss: 0.9368 - accuracy: 0.54 - ETA: 0s - loss: 0.9360 - accuracy: 0.54 - ETA: 0s - loss: 0.9361 - accuracy: 0.54 - ETA: 0s - loss: 0.9353 - accuracy: 0.54 - ETA: 0s - loss: 0.9351 - accuracy: 0.54 - ETA: 0s - loss: 0.9346 - accuracy: 0.54 - ETA: 0s - loss: 0.9340 - accuracy: 0.54 - ETA: 0s - loss: 0.9335 - accuracy: 0.54 - ETA: 0s - loss: 0.9337 - accuracy: 0.54 - ETA: 0s - loss: 0.9342 - accuracy: 0.54 - ETA: 0s - loss: 0.9332 - accuracy: 0.54 - ETA: 0s - loss: 0.9333 - accuracy: 0.54 - ETA: 0s - loss: 0.9331 - accuracy: 0.54 - 5s 184us/step - loss: 0.9328 - accuracy: 0.5422\n",
      "Epoch 2/10\n",
      "27200/27200 [==============================] - ETA: 8s - loss: 0.9198 - accuracy: 0.56 - ETA: 6s - loss: 0.9298 - accuracy: 0.55 - ETA: 5s - loss: 0.8951 - accuracy: 0.54 - ETA: 5s - loss: 0.8934 - accuracy: 0.56 - ETA: 5s - loss: 0.8900 - accuracy: 0.56 - ETA: 5s - loss: 0.8954 - accuracy: 0.55 - ETA: 5s - loss: 0.8985 - accuracy: 0.55 - ETA: 4s - loss: 0.8913 - accuracy: 0.56 - ETA: 4s - loss: 0.8930 - accuracy: 0.56 - ETA: 4s - loss: 0.8922 - accuracy: 0.56 - ETA: 4s - loss: 0.8876 - accuracy: 0.56 - ETA: 4s - loss: 0.8925 - accuracy: 0.56 - ETA: 4s - loss: 0.8947 - accuracy: 0.56 - ETA: 4s - loss: 0.8937 - accuracy: 0.56 - ETA: 4s - loss: 0.8967 - accuracy: 0.56 - ETA: 4s - loss: 0.8975 - accuracy: 0.55 - ETA: 4s - loss: 0.9017 - accuracy: 0.55 - ETA: 4s - loss: 0.9023 - accuracy: 0.56 - ETA: 4s - loss: 0.9014 - accuracy: 0.56 - ETA: 4s - loss: 0.9020 - accuracy: 0.56 - ETA: 3s - loss: 0.9042 - accuracy: 0.55 - ETA: 3s - loss: 0.9080 - accuracy: 0.55 - ETA: 3s - loss: 0.9097 - accuracy: 0.55 - ETA: 3s - loss: 0.9101 - accuracy: 0.55 - ETA: 3s - loss: 0.9097 - accuracy: 0.55 - ETA: 3s - loss: 0.9111 - accuracy: 0.55 - ETA: 3s - loss: 0.9122 - accuracy: 0.55 - ETA: 3s - loss: 0.9119 - accuracy: 0.55 - ETA: 3s - loss: 0.9125 - accuracy: 0.55 - ETA: 3s - loss: 0.9143 - accuracy: 0.55 - ETA: 3s - loss: 0.9162 - accuracy: 0.55 - ETA: 3s - loss: 0.9171 - accuracy: 0.54 - ETA: 3s - loss: 0.9154 - accuracy: 0.54 - ETA: 3s - loss: 0.9151 - accuracy: 0.54 - ETA: 3s - loss: 0.9141 - accuracy: 0.54 - ETA: 3s - loss: 0.9146 - accuracy: 0.55 - ETA: 3s - loss: 0.9151 - accuracy: 0.54 - ETA: 3s - loss: 0.9153 - accuracy: 0.55 - ETA: 2s - loss: 0.9146 - accuracy: 0.55 - ETA: 2s - loss: 0.9143 - accuracy: 0.55 - ETA: 2s - loss: 0.9129 - accuracy: 0.55 - ETA: 2s - loss: 0.9132 - accuracy: 0.55 - ETA: 2s - loss: 0.9136 - accuracy: 0.55 - ETA: 2s - loss: 0.9138 - accuracy: 0.55 - ETA: 2s - loss: 0.9138 - accuracy: 0.55 - ETA: 2s - loss: 0.9137 - accuracy: 0.55 - ETA: 2s - loss: 0.9137 - accuracy: 0.55 - ETA: 2s - loss: 0.9134 - accuracy: 0.55 - ETA: 2s - loss: 0.9131 - accuracy: 0.55 - ETA: 2s - loss: 0.9121 - accuracy: 0.55 - ETA: 2s - loss: 0.9110 - accuracy: 0.55 - ETA: 2s - loss: 0.9102 - accuracy: 0.55 - ETA: 2s - loss: 0.9105 - accuracy: 0.55 - ETA: 2s - loss: 0.9122 - accuracy: 0.55 - ETA: 2s - loss: 0.9121 - accuracy: 0.55 - ETA: 2s - loss: 0.9125 - accuracy: 0.55 - ETA: 1s - loss: 0.9117 - accuracy: 0.55 - ETA: 1s - loss: 0.9118 - accuracy: 0.55 - ETA: 1s - loss: 0.9117 - accuracy: 0.55 - ETA: 1s - loss: 0.9125 - accuracy: 0.55 - ETA: 1s - loss: 0.9124 - accuracy: 0.55 - ETA: 1s - loss: 0.9126 - accuracy: 0.55 - ETA: 1s - loss: 0.9125 - accuracy: 0.55 - ETA: 1s - loss: 0.9127 - accuracy: 0.55 - ETA: 1s - loss: 0.9123 - accuracy: 0.55 - ETA: 1s - loss: 0.9122 - accuracy: 0.55 - ETA: 1s - loss: 0.9124 - accuracy: 0.55 - ETA: 1s - loss: 0.9122 - accuracy: 0.55 - ETA: 1s - loss: 0.9120 - accuracy: 0.55 - ETA: 1s - loss: 0.9117 - accuracy: 0.55 - ETA: 1s - loss: 0.9108 - accuracy: 0.55 - ETA: 1s - loss: 0.9110 - accuracy: 0.55 - ETA: 1s - loss: 0.9123 - accuracy: 0.55 - ETA: 1s - loss: 0.9128 - accuracy: 0.55 - ETA: 1s - loss: 0.9126 - accuracy: 0.55 - ETA: 1s - loss: 0.9133 - accuracy: 0.55 - ETA: 1s - loss: 0.9140 - accuracy: 0.55 - ETA: 0s - loss: 0.9141 - accuracy: 0.55 - ETA: 0s - loss: 0.9143 - accuracy: 0.55 - ETA: 0s - loss: 0.9149 - accuracy: 0.55 - ETA: 0s - loss: 0.9144 - accuracy: 0.55 - ETA: 0s - loss: 0.9141 - accuracy: 0.55 - ETA: 0s - loss: 0.9139 - accuracy: 0.55 - ETA: 0s - loss: 0.9137 - accuracy: 0.55 - ETA: 0s - loss: 0.9131 - accuracy: 0.55 - ETA: 0s - loss: 0.9128 - accuracy: 0.55 - ETA: 0s - loss: 0.9127 - accuracy: 0.55 - ETA: 0s - loss: 0.9122 - accuracy: 0.55 - ETA: 0s - loss: 0.9119 - accuracy: 0.55 - ETA: 0s - loss: 0.9126 - accuracy: 0.55 - ETA: 0s - loss: 0.9122 - accuracy: 0.55 - ETA: 0s - loss: 0.9125 - accuracy: 0.55 - ETA: 0s - loss: 0.9124 - accuracy: 0.55 - ETA: 0s - loss: 0.9127 - accuracy: 0.55 - ETA: 0s - loss: 0.9128 - accuracy: 0.55 - ETA: 0s - loss: 0.9130 - accuracy: 0.55 - ETA: 0s - loss: 0.9132 - accuracy: 0.55 - 5s 183us/step - loss: 0.9130 - accuracy: 0.5569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "27200/27200 [==============================] - ETA: 6s - loss: 1.0071 - accuracy: 0.56 - ETA: 5s - loss: 0.8758 - accuracy: 0.61 - ETA: 5s - loss: 0.8838 - accuracy: 0.58 - ETA: 5s - loss: 0.9027 - accuracy: 0.57 - ETA: 4s - loss: 0.9086 - accuracy: 0.57 - ETA: 4s - loss: 0.9137 - accuracy: 0.56 - ETA: 4s - loss: 0.9079 - accuracy: 0.56 - ETA: 4s - loss: 0.9059 - accuracy: 0.57 - ETA: 4s - loss: 0.9048 - accuracy: 0.57 - ETA: 4s - loss: 0.9127 - accuracy: 0.56 - ETA: 4s - loss: 0.9113 - accuracy: 0.56 - ETA: 4s - loss: 0.9133 - accuracy: 0.56 - ETA: 3s - loss: 0.9075 - accuracy: 0.56 - ETA: 3s - loss: 0.9072 - accuracy: 0.56 - ETA: 3s - loss: 0.9079 - accuracy: 0.56 - ETA: 3s - loss: 0.9081 - accuracy: 0.56 - ETA: 3s - loss: 0.9072 - accuracy: 0.56 - ETA: 3s - loss: 0.9102 - accuracy: 0.56 - ETA: 3s - loss: 0.9139 - accuracy: 0.56 - ETA: 3s - loss: 0.9151 - accuracy: 0.56 - ETA: 3s - loss: 0.9152 - accuracy: 0.56 - ETA: 3s - loss: 0.9136 - accuracy: 0.56 - ETA: 3s - loss: 0.9147 - accuracy: 0.56 - ETA: 3s - loss: 0.9117 - accuracy: 0.56 - ETA: 3s - loss: 0.9103 - accuracy: 0.56 - ETA: 3s - loss: 0.9073 - accuracy: 0.56 - ETA: 3s - loss: 0.9069 - accuracy: 0.56 - ETA: 3s - loss: 0.9074 - accuracy: 0.56 - ETA: 2s - loss: 0.9084 - accuracy: 0.56 - ETA: 2s - loss: 0.9095 - accuracy: 0.56 - ETA: 2s - loss: 0.9079 - accuracy: 0.56 - ETA: 2s - loss: 0.9071 - accuracy: 0.56 - ETA: 2s - loss: 0.9076 - accuracy: 0.56 - ETA: 2s - loss: 0.9071 - accuracy: 0.56 - ETA: 2s - loss: 0.9089 - accuracy: 0.56 - ETA: 2s - loss: 0.9074 - accuracy: 0.56 - ETA: 2s - loss: 0.9082 - accuracy: 0.56 - ETA: 2s - loss: 0.9085 - accuracy: 0.56 - ETA: 2s - loss: 0.9081 - accuracy: 0.56 - ETA: 2s - loss: 0.9071 - accuracy: 0.56 - ETA: 2s - loss: 0.9083 - accuracy: 0.56 - ETA: 2s - loss: 0.9087 - accuracy: 0.56 - ETA: 2s - loss: 0.9091 - accuracy: 0.56 - ETA: 2s - loss: 0.9088 - accuracy: 0.56 - ETA: 2s - loss: 0.9089 - accuracy: 0.56 - ETA: 2s - loss: 0.9082 - accuracy: 0.56 - ETA: 2s - loss: 0.9075 - accuracy: 0.56 - ETA: 2s - loss: 0.9079 - accuracy: 0.56 - ETA: 2s - loss: 0.9079 - accuracy: 0.56 - ETA: 1s - loss: 0.9079 - accuracy: 0.56 - ETA: 1s - loss: 0.9070 - accuracy: 0.56 - ETA: 1s - loss: 0.9064 - accuracy: 0.56 - ETA: 1s - loss: 0.9076 - accuracy: 0.56 - ETA: 1s - loss: 0.9083 - accuracy: 0.56 - ETA: 1s - loss: 0.9091 - accuracy: 0.56 - ETA: 1s - loss: 0.9086 - accuracy: 0.56 - ETA: 1s - loss: 0.9088 - accuracy: 0.56 - ETA: 1s - loss: 0.9085 - accuracy: 0.56 - ETA: 1s - loss: 0.9077 - accuracy: 0.56 - ETA: 1s - loss: 0.9068 - accuracy: 0.56 - ETA: 1s - loss: 0.9067 - accuracy: 0.56 - ETA: 1s - loss: 0.9065 - accuracy: 0.56 - ETA: 1s - loss: 0.9074 - accuracy: 0.56 - ETA: 1s - loss: 0.9070 - accuracy: 0.56 - ETA: 1s - loss: 0.9067 - accuracy: 0.56 - ETA: 1s - loss: 0.9068 - accuracy: 0.56 - ETA: 1s - loss: 0.9071 - accuracy: 0.56 - ETA: 1s - loss: 0.9079 - accuracy: 0.56 - ETA: 1s - loss: 0.9077 - accuracy: 0.56 - ETA: 0s - loss: 0.9076 - accuracy: 0.56 - ETA: 0s - loss: 0.9083 - accuracy: 0.56 - ETA: 0s - loss: 0.9085 - accuracy: 0.56 - ETA: 0s - loss: 0.9082 - accuracy: 0.56 - ETA: 0s - loss: 0.9072 - accuracy: 0.56 - ETA: 0s - loss: 0.9072 - accuracy: 0.56 - ETA: 0s - loss: 0.9064 - accuracy: 0.56 - ETA: 0s - loss: 0.9061 - accuracy: 0.56 - ETA: 0s - loss: 0.9061 - accuracy: 0.56 - ETA: 0s - loss: 0.9058 - accuracy: 0.56 - ETA: 0s - loss: 0.9052 - accuracy: 0.56 - ETA: 0s - loss: 0.9050 - accuracy: 0.56 - ETA: 0s - loss: 0.9046 - accuracy: 0.56 - ETA: 0s - loss: 0.9046 - accuracy: 0.56 - ETA: 0s - loss: 0.9043 - accuracy: 0.56 - ETA: 0s - loss: 0.9046 - accuracy: 0.56 - ETA: 0s - loss: 0.9044 - accuracy: 0.56 - ETA: 0s - loss: 0.9045 - accuracy: 0.56 - ETA: 0s - loss: 0.9047 - accuracy: 0.56 - 5s 166us/step - loss: 0.9047 - accuracy: 0.5637\n",
      "Epoch 4/10\n",
      "27200/27200 [==============================] - ETA: 6s - loss: 1.0813 - accuracy: 0.43 - ETA: 4s - loss: 0.9189 - accuracy: 0.51 - ETA: 5s - loss: 0.9242 - accuracy: 0.51 - ETA: 5s - loss: 0.9390 - accuracy: 0.52 - ETA: 5s - loss: 0.9266 - accuracy: 0.52 - ETA: 5s - loss: 0.9089 - accuracy: 0.53 - ETA: 4s - loss: 0.9102 - accuracy: 0.54 - ETA: 4s - loss: 0.9033 - accuracy: 0.54 - ETA: 4s - loss: 0.9061 - accuracy: 0.54 - ETA: 4s - loss: 0.9002 - accuracy: 0.55 - ETA: 4s - loss: 0.9001 - accuracy: 0.55 - ETA: 4s - loss: 0.9062 - accuracy: 0.55 - ETA: 4s - loss: 0.9071 - accuracy: 0.54 - ETA: 4s - loss: 0.9099 - accuracy: 0.54 - ETA: 4s - loss: 0.9060 - accuracy: 0.54 - ETA: 4s - loss: 0.9078 - accuracy: 0.54 - ETA: 4s - loss: 0.9098 - accuracy: 0.54 - ETA: 4s - loss: 0.9085 - accuracy: 0.54 - ETA: 4s - loss: 0.9121 - accuracy: 0.54 - ETA: 3s - loss: 0.9124 - accuracy: 0.54 - ETA: 3s - loss: 0.9113 - accuracy: 0.54 - ETA: 3s - loss: 0.9082 - accuracy: 0.54 - ETA: 3s - loss: 0.9068 - accuracy: 0.55 - ETA: 3s - loss: 0.9046 - accuracy: 0.55 - ETA: 3s - loss: 0.9049 - accuracy: 0.55 - ETA: 3s - loss: 0.9043 - accuracy: 0.55 - ETA: 3s - loss: 0.9034 - accuracy: 0.55 - ETA: 3s - loss: 0.9023 - accuracy: 0.55 - ETA: 3s - loss: 0.9014 - accuracy: 0.55 - ETA: 3s - loss: 0.9007 - accuracy: 0.55 - ETA: 3s - loss: 0.9003 - accuracy: 0.55 - ETA: 2s - loss: 0.9003 - accuracy: 0.55 - ETA: 2s - loss: 0.8990 - accuracy: 0.55 - ETA: 2s - loss: 0.8977 - accuracy: 0.55 - ETA: 2s - loss: 0.8983 - accuracy: 0.55 - ETA: 2s - loss: 0.8989 - accuracy: 0.55 - ETA: 2s - loss: 0.8988 - accuracy: 0.55 - ETA: 2s - loss: 0.8990 - accuracy: 0.55 - ETA: 2s - loss: 0.8981 - accuracy: 0.55 - ETA: 2s - loss: 0.8982 - accuracy: 0.55 - ETA: 2s - loss: 0.9006 - accuracy: 0.55 - ETA: 2s - loss: 0.9005 - accuracy: 0.55 - ETA: 2s - loss: 0.9005 - accuracy: 0.55 - ETA: 2s - loss: 0.8994 - accuracy: 0.56 - ETA: 2s - loss: 0.8995 - accuracy: 0.56 - ETA: 2s - loss: 0.8995 - accuracy: 0.56 - ETA: 2s - loss: 0.9001 - accuracy: 0.56 - ETA: 2s - loss: 0.9007 - accuracy: 0.56 - ETA: 1s - loss: 0.8995 - accuracy: 0.56 - ETA: 1s - loss: 0.8992 - accuracy: 0.56 - ETA: 1s - loss: 0.8985 - accuracy: 0.56 - ETA: 1s - loss: 0.8982 - accuracy: 0.56 - ETA: 1s - loss: 0.8977 - accuracy: 0.56 - ETA: 1s - loss: 0.8976 - accuracy: 0.56 - ETA: 1s - loss: 0.8983 - accuracy: 0.56 - ETA: 1s - loss: 0.8982 - accuracy: 0.56 - ETA: 1s - loss: 0.8986 - accuracy: 0.56 - ETA: 1s - loss: 0.8999 - accuracy: 0.56 - ETA: 1s - loss: 0.8994 - accuracy: 0.56 - ETA: 1s - loss: 0.9007 - accuracy: 0.56 - ETA: 1s - loss: 0.9003 - accuracy: 0.56 - ETA: 1s - loss: 0.9001 - accuracy: 0.56 - ETA: 1s - loss: 0.9006 - accuracy: 0.56 - ETA: 1s - loss: 0.9005 - accuracy: 0.56 - ETA: 1s - loss: 0.9012 - accuracy: 0.55 - ETA: 0s - loss: 0.9010 - accuracy: 0.55 - ETA: 0s - loss: 0.9012 - accuracy: 0.56 - ETA: 0s - loss: 0.9016 - accuracy: 0.55 - ETA: 0s - loss: 0.9016 - accuracy: 0.56 - ETA: 0s - loss: 0.9009 - accuracy: 0.56 - ETA: 0s - loss: 0.9016 - accuracy: 0.56 - ETA: 0s - loss: 0.9019 - accuracy: 0.56 - ETA: 0s - loss: 0.9015 - accuracy: 0.56 - ETA: 0s - loss: 0.9010 - accuracy: 0.56 - ETA: 0s - loss: 0.9004 - accuracy: 0.56 - ETA: 0s - loss: 0.9013 - accuracy: 0.56 - ETA: 0s - loss: 0.9014 - accuracy: 0.56 - ETA: 0s - loss: 0.9007 - accuracy: 0.56 - ETA: 0s - loss: 0.9004 - accuracy: 0.56 - ETA: 0s - loss: 0.9006 - accuracy: 0.56 - ETA: 0s - loss: 0.9000 - accuracy: 0.56 - ETA: 0s - loss: 0.9003 - accuracy: 0.56 - ETA: 0s - loss: 0.8997 - accuracy: 0.56 - ETA: 0s - loss: 0.8998 - accuracy: 0.56 - ETA: 0s - loss: 0.8998 - accuracy: 0.56 - ETA: 0s - loss: 0.8999 - accuracy: 0.56 - ETA: 0s - loss: 0.8997 - accuracy: 0.56 - ETA: 0s - loss: 0.8993 - accuracy: 0.56 - ETA: 0s - loss: 0.8992 - accuracy: 0.56 - ETA: 0s - loss: 0.8990 - accuracy: 0.56 - ETA: 0s - loss: 0.8991 - accuracy: 0.56 - ETA: 0s - loss: 0.8993 - accuracy: 0.56 - 5s 173us/step - loss: 0.8992 - accuracy: 0.5632\n",
      "Epoch 5/10\n",
      "27200/27200 [==============================] - ETA: 10s - loss: 0.8438 - accuracy: 0.562 - ETA: 10s - loss: 0.8823 - accuracy: 0.575 - ETA: 10s - loss: 0.9037 - accuracy: 0.552 - ETA: 9s - loss: 0.9033 - accuracy: 0.564 - ETA: 10s - loss: 0.8919 - accuracy: 0.576 - ETA: 9s - loss: 0.9038 - accuracy: 0.568 - ETA: 10s - loss: 0.9117 - accuracy: 0.563 - ETA: 10s - loss: 0.8979 - accuracy: 0.578 - ETA: 10s - loss: 0.8938 - accuracy: 0.577 - ETA: 10s - loss: 0.9000 - accuracy: 0.571 - ETA: 9s - loss: 0.8926 - accuracy: 0.577 - ETA: 10s - loss: 0.8973 - accuracy: 0.571 - ETA: 9s - loss: 0.8936 - accuracy: 0.576 - ETA: 9s - loss: 0.8947 - accuracy: 0.57 - ETA: 9s - loss: 0.8950 - accuracy: 0.57 - ETA: 9s - loss: 0.8968 - accuracy: 0.57 - ETA: 8s - loss: 0.8951 - accuracy: 0.57 - ETA: 7s - loss: 0.8946 - accuracy: 0.57 - ETA: 7s - loss: 0.8980 - accuracy: 0.56 - ETA: 6s - loss: 0.8975 - accuracy: 0.56 - ETA: 6s - loss: 0.8960 - accuracy: 0.56 - ETA: 6s - loss: 0.8939 - accuracy: 0.56 - ETA: 5s - loss: 0.8924 - accuracy: 0.56 - ETA: 5s - loss: 0.8931 - accuracy: 0.56 - ETA: 5s - loss: 0.8935 - accuracy: 0.56 - ETA: 5s - loss: 0.8918 - accuracy: 0.56 - ETA: 5s - loss: 0.8909 - accuracy: 0.56 - ETA: 5s - loss: 0.8914 - accuracy: 0.56 - ETA: 5s - loss: 0.8927 - accuracy: 0.56 - ETA: 5s - loss: 0.8939 - accuracy: 0.56 - ETA: 5s - loss: 0.8935 - accuracy: 0.56 - ETA: 5s - loss: 0.8940 - accuracy: 0.56 - ETA: 5s - loss: 0.8939 - accuracy: 0.56 - ETA: 5s - loss: 0.8939 - accuracy: 0.56 - ETA: 5s - loss: 0.8931 - accuracy: 0.56 - ETA: 5s - loss: 0.8948 - accuracy: 0.56 - ETA: 5s - loss: 0.8943 - accuracy: 0.56 - ETA: 5s - loss: 0.8948 - accuracy: 0.56 - ETA: 5s - loss: 0.8960 - accuracy: 0.56 - ETA: 5s - loss: 0.8959 - accuracy: 0.56 - ETA: 5s - loss: 0.8967 - accuracy: 0.56 - ETA: 5s - loss: 0.8963 - accuracy: 0.56 - ETA: 6s - loss: 0.8954 - accuracy: 0.56 - ETA: 6s - loss: 0.8960 - accuracy: 0.56 - ETA: 6s - loss: 0.8962 - accuracy: 0.56 - ETA: 6s - loss: 0.8980 - accuracy: 0.56 - ETA: 6s - loss: 0.8976 - accuracy: 0.56 - ETA: 6s - loss: 0.8975 - accuracy: 0.56 - ETA: 6s - loss: 0.8978 - accuracy: 0.56 - ETA: 6s - loss: 0.8972 - accuracy: 0.56 - ETA: 6s - loss: 0.8966 - accuracy: 0.56 - ETA: 6s - loss: 0.8968 - accuracy: 0.56 - ETA: 6s - loss: 0.8952 - accuracy: 0.56 - ETA: 6s - loss: 0.8955 - accuracy: 0.56 - ETA: 6s - loss: 0.8943 - accuracy: 0.56 - ETA: 6s - loss: 0.8949 - accuracy: 0.56 - ETA: 6s - loss: 0.8947 - accuracy: 0.56 - ETA: 6s - loss: 0.8939 - accuracy: 0.56 - ETA: 6s - loss: 0.8927 - accuracy: 0.56 - ETA: 6s - loss: 0.8925 - accuracy: 0.56 - ETA: 6s - loss: 0.8909 - accuracy: 0.56 - ETA: 5s - loss: 0.8929 - accuracy: 0.56 - ETA: 5s - loss: 0.8940 - accuracy: 0.56 - ETA: 5s - loss: 0.8932 - accuracy: 0.56 - ETA: 5s - loss: 0.8942 - accuracy: 0.56 - ETA: 5s - loss: 0.8941 - accuracy: 0.56 - ETA: 5s - loss: 0.8942 - accuracy: 0.56 - ETA: 5s - loss: 0.8932 - accuracy: 0.56 - ETA: 5s - loss: 0.8919 - accuracy: 0.56 - ETA: 5s - loss: 0.8916 - accuracy: 0.56 - ETA: 5s - loss: 0.8918 - accuracy: 0.56 - ETA: 5s - loss: 0.8922 - accuracy: 0.56 - ETA: 5s - loss: 0.8924 - accuracy: 0.56 - ETA: 4s - loss: 0.8930 - accuracy: 0.56 - ETA: 4s - loss: 0.8926 - accuracy: 0.56 - ETA: 4s - loss: 0.8925 - accuracy: 0.56 - ETA: 4s - loss: 0.8924 - accuracy: 0.56 - ETA: 4s - loss: 0.8918 - accuracy: 0.56 - ETA: 4s - loss: 0.8924 - accuracy: 0.56 - ETA: 4s - loss: 0.8919 - accuracy: 0.56 - ETA: 3s - loss: 0.8933 - accuracy: 0.56 - ETA: 3s - loss: 0.8932 - accuracy: 0.56 - ETA: 3s - loss: 0.8926 - accuracy: 0.56 - ETA: 3s - loss: 0.8934 - accuracy: 0.56 - ETA: 3s - loss: 0.8936 - accuracy: 0.56 - ETA: 3s - loss: 0.8939 - accuracy: 0.56 - ETA: 3s - loss: 0.8940 - accuracy: 0.56 - ETA: 3s - loss: 0.8935 - accuracy: 0.56 - ETA: 2s - loss: 0.8933 - accuracy: 0.56 - ETA: 2s - loss: 0.8928 - accuracy: 0.56 - ETA: 2s - loss: 0.8929 - accuracy: 0.56 - ETA: 2s - loss: 0.8917 - accuracy: 0.56 - ETA: 2s - loss: 0.8925 - accuracy: 0.56 - ETA: 2s - loss: 0.8924 - accuracy: 0.56 - ETA: 2s - loss: 0.8919 - accuracy: 0.56 - ETA: 2s - loss: 0.8925 - accuracy: 0.56 - ETA: 1s - loss: 0.8923 - accuracy: 0.56 - ETA: 1s - loss: 0.8929 - accuracy: 0.56 - ETA: 1s - loss: 0.8941 - accuracy: 0.56 - ETA: 1s - loss: 0.8945 - accuracy: 0.56 - ETA: 1s - loss: 0.8943 - accuracy: 0.56 - ETA: 1s - loss: 0.8940 - accuracy: 0.56 - ETA: 1s - loss: 0.8948 - accuracy: 0.56 - ETA: 1s - loss: 0.8942 - accuracy: 0.56 - ETA: 1s - loss: 0.8941 - accuracy: 0.56 - ETA: 1s - loss: 0.8938 - accuracy: 0.56 - ETA: 1s - loss: 0.8939 - accuracy: 0.56 - ETA: 0s - loss: 0.8939 - accuracy: 0.56 - ETA: 0s - loss: 0.8944 - accuracy: 0.56 - ETA: 0s - loss: 0.8942 - accuracy: 0.56 - ETA: 0s - loss: 0.8948 - accuracy: 0.56 - ETA: 0s - loss: 0.8945 - accuracy: 0.56 - ETA: 0s - loss: 0.8947 - accuracy: 0.56 - ETA: 0s - loss: 0.8947 - accuracy: 0.56 - ETA: 0s - loss: 0.8945 - accuracy: 0.56 - ETA: 0s - loss: 0.8954 - accuracy: 0.56 - ETA: 0s - loss: 0.8950 - accuracy: 0.56 - ETA: 0s - loss: 0.8944 - accuracy: 0.56 - ETA: 0s - loss: 0.8944 - accuracy: 0.56 - 6s 230us/step - loss: 0.8947 - accuracy: 0.5663\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27200/27200 [==============================] - ETA: 10s - loss: 0.8517 - accuracy: 0.625 - ETA: 7s - loss: 0.8626 - accuracy: 0.572 - ETA: 6s - loss: 0.8488 - accuracy: 0.58 - ETA: 6s - loss: 0.8740 - accuracy: 0.57 - ETA: 5s - loss: 0.8674 - accuracy: 0.58 - ETA: 5s - loss: 0.8689 - accuracy: 0.57 - ETA: 5s - loss: 0.8698 - accuracy: 0.58 - ETA: 5s - loss: 0.8746 - accuracy: 0.58 - ETA: 4s - loss: 0.8804 - accuracy: 0.57 - ETA: 4s - loss: 0.8801 - accuracy: 0.57 - ETA: 4s - loss: 0.8776 - accuracy: 0.57 - ETA: 4s - loss: 0.8758 - accuracy: 0.57 - ETA: 4s - loss: 0.8741 - accuracy: 0.57 - ETA: 4s - loss: 0.8722 - accuracy: 0.57 - ETA: 3s - loss: 0.8751 - accuracy: 0.57 - ETA: 3s - loss: 0.8764 - accuracy: 0.57 - ETA: 3s - loss: 0.8773 - accuracy: 0.57 - ETA: 3s - loss: 0.8766 - accuracy: 0.57 - ETA: 3s - loss: 0.8793 - accuracy: 0.57 - ETA: 3s - loss: 0.8797 - accuracy: 0.57 - ETA: 3s - loss: 0.8792 - accuracy: 0.57 - ETA: 3s - loss: 0.8839 - accuracy: 0.57 - ETA: 3s - loss: 0.8853 - accuracy: 0.57 - ETA: 3s - loss: 0.8860 - accuracy: 0.56 - ETA: 3s - loss: 0.8849 - accuracy: 0.57 - ETA: 3s - loss: 0.8839 - accuracy: 0.57 - ETA: 3s - loss: 0.8828 - accuracy: 0.56 - ETA: 2s - loss: 0.8801 - accuracy: 0.57 - ETA: 2s - loss: 0.8802 - accuracy: 0.57 - ETA: 2s - loss: 0.8806 - accuracy: 0.57 - ETA: 2s - loss: 0.8818 - accuracy: 0.57 - ETA: 2s - loss: 0.8826 - accuracy: 0.57 - ETA: 2s - loss: 0.8834 - accuracy: 0.57 - ETA: 2s - loss: 0.8855 - accuracy: 0.56 - ETA: 2s - loss: 0.8867 - accuracy: 0.57 - ETA: 2s - loss: 0.8869 - accuracy: 0.56 - ETA: 2s - loss: 0.8870 - accuracy: 0.56 - ETA: 2s - loss: 0.8867 - accuracy: 0.56 - ETA: 2s - loss: 0.8864 - accuracy: 0.56 - ETA: 2s - loss: 0.8855 - accuracy: 0.56 - ETA: 2s - loss: 0.8847 - accuracy: 0.57 - ETA: 2s - loss: 0.8845 - accuracy: 0.57 - ETA: 2s - loss: 0.8855 - accuracy: 0.57 - ETA: 2s - loss: 0.8849 - accuracy: 0.57 - ETA: 2s - loss: 0.8847 - accuracy: 0.57 - ETA: 2s - loss: 0.8846 - accuracy: 0.57 - ETA: 2s - loss: 0.8849 - accuracy: 0.57 - ETA: 1s - loss: 0.8850 - accuracy: 0.57 - ETA: 1s - loss: 0.8850 - accuracy: 0.57 - ETA: 1s - loss: 0.8852 - accuracy: 0.57 - ETA: 1s - loss: 0.8860 - accuracy: 0.57 - ETA: 1s - loss: 0.8859 - accuracy: 0.57 - ETA: 1s - loss: 0.8860 - accuracy: 0.57 - ETA: 1s - loss: 0.8857 - accuracy: 0.57 - ETA: 1s - loss: 0.8854 - accuracy: 0.57 - ETA: 1s - loss: 0.8855 - accuracy: 0.57 - ETA: 1s - loss: 0.8862 - accuracy: 0.57 - ETA: 1s - loss: 0.8862 - accuracy: 0.57 - ETA: 1s - loss: 0.8868 - accuracy: 0.57 - ETA: 1s - loss: 0.8875 - accuracy: 0.57 - ETA: 1s - loss: 0.8884 - accuracy: 0.57 - ETA: 1s - loss: 0.8885 - accuracy: 0.57 - ETA: 1s - loss: 0.8883 - accuracy: 0.57 - ETA: 1s - loss: 0.8888 - accuracy: 0.57 - ETA: 1s - loss: 0.8884 - accuracy: 0.57 - ETA: 1s - loss: 0.8883 - accuracy: 0.57 - ETA: 1s - loss: 0.8875 - accuracy: 0.57 - ETA: 0s - loss: 0.8873 - accuracy: 0.57 - ETA: 0s - loss: 0.8874 - accuracy: 0.57 - ETA: 0s - loss: 0.8868 - accuracy: 0.57 - ETA: 0s - loss: 0.8875 - accuracy: 0.57 - ETA: 0s - loss: 0.8872 - accuracy: 0.57 - ETA: 0s - loss: 0.8881 - accuracy: 0.57 - ETA: 0s - loss: 0.8886 - accuracy: 0.57 - ETA: 0s - loss: 0.8887 - accuracy: 0.57 - ETA: 0s - loss: 0.8884 - accuracy: 0.57 - ETA: 0s - loss: 0.8890 - accuracy: 0.57 - ETA: 0s - loss: 0.8891 - accuracy: 0.57 - ETA: 0s - loss: 0.8896 - accuracy: 0.57 - ETA: 0s - loss: 0.8897 - accuracy: 0.57 - ETA: 0s - loss: 0.8899 - accuracy: 0.57 - ETA: 0s - loss: 0.8898 - accuracy: 0.57 - ETA: 0s - loss: 0.8902 - accuracy: 0.57 - ETA: 0s - loss: 0.8899 - accuracy: 0.57 - ETA: 0s - loss: 0.8898 - accuracy: 0.57 - ETA: 0s - loss: 0.8899 - accuracy: 0.57 - 4s 162us/step - loss: 0.8901 - accuracy: 0.5713\n",
      "Epoch 7/10\n",
      "27200/27200 [==============================] - ETA: 8s - loss: 0.8540 - accuracy: 0.50 - ETA: 6s - loss: 0.8780 - accuracy: 0.57 - ETA: 5s - loss: 0.8998 - accuracy: 0.58 - ETA: 5s - loss: 0.8716 - accuracy: 0.60 - ETA: 4s - loss: 0.8715 - accuracy: 0.59 - ETA: 4s - loss: 0.8725 - accuracy: 0.58 - ETA: 4s - loss: 0.8755 - accuracy: 0.59 - ETA: 4s - loss: 0.8778 - accuracy: 0.58 - ETA: 4s - loss: 0.8770 - accuracy: 0.58 - ETA: 3s - loss: 0.8752 - accuracy: 0.58 - ETA: 3s - loss: 0.8738 - accuracy: 0.58 - ETA: 3s - loss: 0.8821 - accuracy: 0.57 - ETA: 3s - loss: 0.8755 - accuracy: 0.58 - ETA: 3s - loss: 0.8769 - accuracy: 0.58 - ETA: 3s - loss: 0.8787 - accuracy: 0.57 - ETA: 3s - loss: 0.8796 - accuracy: 0.57 - ETA: 3s - loss: 0.8816 - accuracy: 0.57 - ETA: 3s - loss: 0.8828 - accuracy: 0.57 - ETA: 3s - loss: 0.8805 - accuracy: 0.57 - ETA: 3s - loss: 0.8803 - accuracy: 0.57 - ETA: 3s - loss: 0.8812 - accuracy: 0.57 - ETA: 3s - loss: 0.8801 - accuracy: 0.57 - ETA: 3s - loss: 0.8788 - accuracy: 0.57 - ETA: 3s - loss: 0.8789 - accuracy: 0.57 - ETA: 3s - loss: 0.8789 - accuracy: 0.57 - ETA: 3s - loss: 0.8773 - accuracy: 0.58 - ETA: 3s - loss: 0.8769 - accuracy: 0.58 - ETA: 3s - loss: 0.8773 - accuracy: 0.57 - ETA: 3s - loss: 0.8766 - accuracy: 0.57 - ETA: 3s - loss: 0.8762 - accuracy: 0.58 - ETA: 3s - loss: 0.8765 - accuracy: 0.58 - ETA: 3s - loss: 0.8768 - accuracy: 0.57 - ETA: 3s - loss: 0.8763 - accuracy: 0.58 - ETA: 3s - loss: 0.8772 - accuracy: 0.58 - ETA: 3s - loss: 0.8776 - accuracy: 0.57 - ETA: 3s - loss: 0.8785 - accuracy: 0.57 - ETA: 3s - loss: 0.8798 - accuracy: 0.57 - ETA: 4s - loss: 0.8795 - accuracy: 0.57 - ETA: 4s - loss: 0.8804 - accuracy: 0.57 - ETA: 4s - loss: 0.8817 - accuracy: 0.57 - ETA: 4s - loss: 0.8814 - accuracy: 0.57 - ETA: 4s - loss: 0.8816 - accuracy: 0.57 - ETA: 4s - loss: 0.8819 - accuracy: 0.57 - ETA: 4s - loss: 0.8815 - accuracy: 0.57 - ETA: 4s - loss: 0.8824 - accuracy: 0.57 - ETA: 4s - loss: 0.8818 - accuracy: 0.57 - ETA: 4s - loss: 0.8808 - accuracy: 0.57 - ETA: 4s - loss: 0.8813 - accuracy: 0.57 - ETA: 4s - loss: 0.8815 - accuracy: 0.57 - ETA: 4s - loss: 0.8814 - accuracy: 0.57 - ETA: 4s - loss: 0.8814 - accuracy: 0.57 - ETA: 4s - loss: 0.8818 - accuracy: 0.57 - ETA: 4s - loss: 0.8822 - accuracy: 0.57 - ETA: 4s - loss: 0.8825 - accuracy: 0.57 - ETA: 4s - loss: 0.8816 - accuracy: 0.57 - ETA: 4s - loss: 0.8809 - accuracy: 0.57 - ETA: 4s - loss: 0.8812 - accuracy: 0.57 - ETA: 4s - loss: 0.8814 - accuracy: 0.57 - ETA: 4s - loss: 0.8810 - accuracy: 0.57 - ETA: 4s - loss: 0.8815 - accuracy: 0.57 - ETA: 4s - loss: 0.8813 - accuracy: 0.57 - ETA: 4s - loss: 0.8816 - accuracy: 0.57 - ETA: 4s - loss: 0.8818 - accuracy: 0.57 - ETA: 4s - loss: 0.8817 - accuracy: 0.57 - ETA: 4s - loss: 0.8825 - accuracy: 0.57 - ETA: 4s - loss: 0.8826 - accuracy: 0.57 - ETA: 4s - loss: 0.8823 - accuracy: 0.57 - ETA: 4s - loss: 0.8821 - accuracy: 0.57 - ETA: 4s - loss: 0.8821 - accuracy: 0.57 - ETA: 4s - loss: 0.8816 - accuracy: 0.57 - ETA: 4s - loss: 0.8814 - accuracy: 0.57 - ETA: 3s - loss: 0.8813 - accuracy: 0.57 - ETA: 3s - loss: 0.8831 - accuracy: 0.57 - ETA: 3s - loss: 0.8829 - accuracy: 0.57 - ETA: 3s - loss: 0.8828 - accuracy: 0.57 - ETA: 3s - loss: 0.8830 - accuracy: 0.57 - ETA: 3s - loss: 0.8827 - accuracy: 0.57 - ETA: 3s - loss: 0.8830 - accuracy: 0.57 - ETA: 3s - loss: 0.8831 - accuracy: 0.57 - ETA: 3s - loss: 0.8830 - accuracy: 0.57 - ETA: 3s - loss: 0.8833 - accuracy: 0.57 - ETA: 2s - loss: 0.8843 - accuracy: 0.57 - ETA: 2s - loss: 0.8850 - accuracy: 0.57 - ETA: 2s - loss: 0.8855 - accuracy: 0.57 - ETA: 2s - loss: 0.8858 - accuracy: 0.57 - ETA: 2s - loss: 0.8859 - accuracy: 0.57 - ETA: 2s - loss: 0.8864 - accuracy: 0.57 - ETA: 2s - loss: 0.8858 - accuracy: 0.57 - ETA: 2s - loss: 0.8868 - accuracy: 0.57 - ETA: 2s - loss: 0.8864 - accuracy: 0.57 - ETA: 2s - loss: 0.8867 - accuracy: 0.57 - ETA: 1s - loss: 0.8861 - accuracy: 0.57 - ETA: 1s - loss: 0.8862 - accuracy: 0.57 - ETA: 1s - loss: 0.8857 - accuracy: 0.57 - ETA: 1s - loss: 0.8859 - accuracy: 0.57 - ETA: 1s - loss: 0.8855 - accuracy: 0.57 - ETA: 1s - loss: 0.8855 - accuracy: 0.57 - ETA: 1s - loss: 0.8860 - accuracy: 0.57 - ETA: 1s - loss: 0.8865 - accuracy: 0.56 - ETA: 1s - loss: 0.8862 - accuracy: 0.56 - ETA: 1s - loss: 0.8860 - accuracy: 0.56 - ETA: 1s - loss: 0.8857 - accuracy: 0.57 - ETA: 0s - loss: 0.8853 - accuracy: 0.57 - ETA: 0s - loss: 0.8859 - accuracy: 0.56 - ETA: 0s - loss: 0.8854 - accuracy: 0.57 - ETA: 0s - loss: 0.8862 - accuracy: 0.57 - ETA: 0s - loss: 0.8862 - accuracy: 0.57 - ETA: 0s - loss: 0.8863 - accuracy: 0.57 - ETA: 0s - loss: 0.8865 - accuracy: 0.57 - ETA: 0s - loss: 0.8863 - accuracy: 0.57 - ETA: 0s - loss: 0.8865 - accuracy: 0.57 - ETA: 0s - loss: 0.8857 - accuracy: 0.57 - ETA: 0s - loss: 0.8865 - accuracy: 0.57 - ETA: 0s - loss: 0.8867 - accuracy: 0.57 - ETA: 0s - loss: 0.8868 - accuracy: 0.57 - 6s 220us/step - loss: 0.8867 - accuracy: 0.5711\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27200/27200 [==============================] - ETA: 8s - loss: 0.8967 - accuracy: 0.56 - ETA: 7s - loss: 0.9296 - accuracy: 0.54 - ETA: 6s - loss: 0.9197 - accuracy: 0.53 - ETA: 5s - loss: 0.9111 - accuracy: 0.55 - ETA: 5s - loss: 0.9082 - accuracy: 0.55 - ETA: 5s - loss: 0.9004 - accuracy: 0.56 - ETA: 5s - loss: 0.8945 - accuracy: 0.56 - ETA: 6s - loss: 0.9011 - accuracy: 0.56 - ETA: 6s - loss: 0.8982 - accuracy: 0.56 - ETA: 7s - loss: 0.8980 - accuracy: 0.56 - ETA: 7s - loss: 0.8998 - accuracy: 0.56 - ETA: 7s - loss: 0.8984 - accuracy: 0.55 - ETA: 7s - loss: 0.8943 - accuracy: 0.56 - ETA: 7s - loss: 0.8904 - accuracy: 0.56 - ETA: 7s - loss: 0.8910 - accuracy: 0.56 - ETA: 7s - loss: 0.8836 - accuracy: 0.56 - ETA: 7s - loss: 0.8812 - accuracy: 0.57 - ETA: 8s - loss: 0.8804 - accuracy: 0.57 - ETA: 8s - loss: 0.8778 - accuracy: 0.57 - ETA: 8s - loss: 0.8798 - accuracy: 0.57 - ETA: 8s - loss: 0.8763 - accuracy: 0.57 - ETA: 8s - loss: 0.8783 - accuracy: 0.57 - ETA: 8s - loss: 0.8816 - accuracy: 0.57 - ETA: 8s - loss: 0.8775 - accuracy: 0.57 - ETA: 8s - loss: 0.8765 - accuracy: 0.57 - ETA: 8s - loss: 0.8770 - accuracy: 0.57 - ETA: 8s - loss: 0.8754 - accuracy: 0.57 - ETA: 8s - loss: 0.8749 - accuracy: 0.57 - ETA: 8s - loss: 0.8760 - accuracy: 0.57 - ETA: 8s - loss: 0.8761 - accuracy: 0.57 - ETA: 8s - loss: 0.8768 - accuracy: 0.58 - ETA: 8s - loss: 0.8753 - accuracy: 0.58 - ETA: 8s - loss: 0.8739 - accuracy: 0.58 - ETA: 8s - loss: 0.8727 - accuracy: 0.58 - ETA: 8s - loss: 0.8712 - accuracy: 0.58 - ETA: 8s - loss: 0.8714 - accuracy: 0.58 - ETA: 8s - loss: 0.8724 - accuracy: 0.58 - ETA: 8s - loss: 0.8715 - accuracy: 0.58 - ETA: 8s - loss: 0.8707 - accuracy: 0.58 - ETA: 8s - loss: 0.8700 - accuracy: 0.58 - ETA: 8s - loss: 0.8705 - accuracy: 0.58 - ETA: 8s - loss: 0.8700 - accuracy: 0.58 - ETA: 8s - loss: 0.8719 - accuracy: 0.58 - ETA: 8s - loss: 0.8713 - accuracy: 0.58 - ETA: 8s - loss: 0.8724 - accuracy: 0.58 - ETA: 7s - loss: 0.8720 - accuracy: 0.58 - ETA: 7s - loss: 0.8726 - accuracy: 0.58 - ETA: 7s - loss: 0.8734 - accuracy: 0.58 - ETA: 7s - loss: 0.8736 - accuracy: 0.58 - ETA: 7s - loss: 0.8747 - accuracy: 0.58 - ETA: 7s - loss: 0.8742 - accuracy: 0.58 - ETA: 7s - loss: 0.8755 - accuracy: 0.58 - ETA: 6s - loss: 0.8763 - accuracy: 0.58 - ETA: 6s - loss: 0.8765 - accuracy: 0.58 - ETA: 6s - loss: 0.8761 - accuracy: 0.58 - ETA: 5s - loss: 0.8759 - accuracy: 0.58 - ETA: 5s - loss: 0.8764 - accuracy: 0.58 - ETA: 5s - loss: 0.8761 - accuracy: 0.58 - ETA: 5s - loss: 0.8776 - accuracy: 0.58 - ETA: 5s - loss: 0.8788 - accuracy: 0.58 - ETA: 5s - loss: 0.8793 - accuracy: 0.58 - ETA: 4s - loss: 0.8789 - accuracy: 0.58 - ETA: 4s - loss: 0.8800 - accuracy: 0.57 - ETA: 4s - loss: 0.8793 - accuracy: 0.57 - ETA: 4s - loss: 0.8802 - accuracy: 0.57 - ETA: 4s - loss: 0.8803 - accuracy: 0.57 - ETA: 4s - loss: 0.8802 - accuracy: 0.57 - ETA: 4s - loss: 0.8801 - accuracy: 0.57 - ETA: 4s - loss: 0.8796 - accuracy: 0.57 - ETA: 3s - loss: 0.8800 - accuracy: 0.57 - ETA: 3s - loss: 0.8815 - accuracy: 0.57 - ETA: 3s - loss: 0.8820 - accuracy: 0.57 - ETA: 3s - loss: 0.8817 - accuracy: 0.57 - ETA: 3s - loss: 0.8819 - accuracy: 0.57 - ETA: 3s - loss: 0.8825 - accuracy: 0.57 - ETA: 3s - loss: 0.8832 - accuracy: 0.57 - ETA: 3s - loss: 0.8826 - accuracy: 0.57 - ETA: 3s - loss: 0.8826 - accuracy: 0.57 - ETA: 2s - loss: 0.8821 - accuracy: 0.57 - ETA: 2s - loss: 0.8832 - accuracy: 0.57 - ETA: 2s - loss: 0.8827 - accuracy: 0.57 - ETA: 2s - loss: 0.8826 - accuracy: 0.57 - ETA: 2s - loss: 0.8831 - accuracy: 0.57 - ETA: 2s - loss: 0.8837 - accuracy: 0.57 - ETA: 2s - loss: 0.8836 - accuracy: 0.57 - ETA: 2s - loss: 0.8841 - accuracy: 0.57 - ETA: 2s - loss: 0.8834 - accuracy: 0.57 - ETA: 2s - loss: 0.8841 - accuracy: 0.57 - ETA: 2s - loss: 0.8842 - accuracy: 0.57 - ETA: 1s - loss: 0.8851 - accuracy: 0.57 - ETA: 1s - loss: 0.8852 - accuracy: 0.57 - ETA: 1s - loss: 0.8848 - accuracy: 0.57 - ETA: 1s - loss: 0.8844 - accuracy: 0.57 - ETA: 1s - loss: 0.8845 - accuracy: 0.57 - ETA: 1s - loss: 0.8842 - accuracy: 0.57 - ETA: 1s - loss: 0.8839 - accuracy: 0.57 - ETA: 1s - loss: 0.8839 - accuracy: 0.57 - ETA: 1s - loss: 0.8839 - accuracy: 0.57 - ETA: 1s - loss: 0.8840 - accuracy: 0.57 - ETA: 1s - loss: 0.8836 - accuracy: 0.57 - ETA: 0s - loss: 0.8835 - accuracy: 0.57 - ETA: 0s - loss: 0.8833 - accuracy: 0.57 - ETA: 0s - loss: 0.8825 - accuracy: 0.57 - ETA: 0s - loss: 0.8826 - accuracy: 0.57 - ETA: 0s - loss: 0.8828 - accuracy: 0.57 - ETA: 0s - loss: 0.8833 - accuracy: 0.57 - ETA: 0s - loss: 0.8838 - accuracy: 0.57 - ETA: 0s - loss: 0.8836 - accuracy: 0.57 - ETA: 0s - loss: 0.8838 - accuracy: 0.57 - ETA: 0s - loss: 0.8839 - accuracy: 0.57 - ETA: 0s - loss: 0.8839 - accuracy: 0.57 - ETA: 0s - loss: 0.8835 - accuracy: 0.57 - ETA: 0s - loss: 0.8837 - accuracy: 0.57 - 6s 216us/step - loss: 0.8835 - accuracy: 0.5739\n",
      "Epoch 9/10\n",
      "27200/27200 [==============================] - ETA: 8s - loss: 0.8303 - accuracy: 0.62 - ETA: 6s - loss: 0.8824 - accuracy: 0.60 - ETA: 6s - loss: 0.8617 - accuracy: 0.60 - ETA: 5s - loss: 0.8813 - accuracy: 0.57 - ETA: 5s - loss: 0.8962 - accuracy: 0.56 - ETA: 5s - loss: 0.8943 - accuracy: 0.56 - ETA: 5s - loss: 0.8893 - accuracy: 0.56 - ETA: 5s - loss: 0.8909 - accuracy: 0.57 - ETA: 4s - loss: 0.8963 - accuracy: 0.56 - ETA: 4s - loss: 0.8914 - accuracy: 0.56 - ETA: 4s - loss: 0.8936 - accuracy: 0.56 - ETA: 4s - loss: 0.8917 - accuracy: 0.56 - ETA: 4s - loss: 0.8929 - accuracy: 0.56 - ETA: 4s - loss: 0.8916 - accuracy: 0.56 - ETA: 4s - loss: 0.8907 - accuracy: 0.56 - ETA: 3s - loss: 0.8915 - accuracy: 0.56 - ETA: 3s - loss: 0.8897 - accuracy: 0.56 - ETA: 3s - loss: 0.8884 - accuracy: 0.56 - ETA: 3s - loss: 0.8876 - accuracy: 0.56 - ETA: 3s - loss: 0.8871 - accuracy: 0.57 - ETA: 3s - loss: 0.8877 - accuracy: 0.56 - ETA: 3s - loss: 0.8864 - accuracy: 0.56 - ETA: 3s - loss: 0.8841 - accuracy: 0.57 - ETA: 3s - loss: 0.8862 - accuracy: 0.57 - ETA: 3s - loss: 0.8863 - accuracy: 0.57 - ETA: 3s - loss: 0.8874 - accuracy: 0.57 - ETA: 3s - loss: 0.8863 - accuracy: 0.57 - ETA: 3s - loss: 0.8851 - accuracy: 0.57 - ETA: 2s - loss: 0.8841 - accuracy: 0.57 - ETA: 2s - loss: 0.8847 - accuracy: 0.57 - ETA: 2s - loss: 0.8847 - accuracy: 0.57 - ETA: 2s - loss: 0.8842 - accuracy: 0.57 - ETA: 2s - loss: 0.8848 - accuracy: 0.57 - ETA: 2s - loss: 0.8842 - accuracy: 0.57 - ETA: 2s - loss: 0.8838 - accuracy: 0.57 - ETA: 2s - loss: 0.8833 - accuracy: 0.57 - ETA: 2s - loss: 0.8832 - accuracy: 0.57 - ETA: 2s - loss: 0.8830 - accuracy: 0.57 - ETA: 2s - loss: 0.8830 - accuracy: 0.57 - ETA: 2s - loss: 0.8832 - accuracy: 0.57 - ETA: 2s - loss: 0.8839 - accuracy: 0.57 - ETA: 2s - loss: 0.8838 - accuracy: 0.57 - ETA: 2s - loss: 0.8838 - accuracy: 0.57 - ETA: 2s - loss: 0.8837 - accuracy: 0.57 - ETA: 3s - loss: 0.8838 - accuracy: 0.57 - ETA: 3s - loss: 0.8837 - accuracy: 0.57 - ETA: 3s - loss: 0.8832 - accuracy: 0.57 - ETA: 3s - loss: 0.8826 - accuracy: 0.57 - ETA: 3s - loss: 0.8820 - accuracy: 0.57 - ETA: 3s - loss: 0.8812 - accuracy: 0.57 - ETA: 3s - loss: 0.8816 - accuracy: 0.57 - ETA: 3s - loss: 0.8817 - accuracy: 0.57 - ETA: 3s - loss: 0.8828 - accuracy: 0.57 - ETA: 3s - loss: 0.8819 - accuracy: 0.57 - ETA: 3s - loss: 0.8823 - accuracy: 0.57 - ETA: 3s - loss: 0.8826 - accuracy: 0.57 - ETA: 3s - loss: 0.8824 - accuracy: 0.57 - ETA: 3s - loss: 0.8826 - accuracy: 0.57 - ETA: 3s - loss: 0.8826 - accuracy: 0.57 - ETA: 3s - loss: 0.8819 - accuracy: 0.57 - ETA: 3s - loss: 0.8821 - accuracy: 0.57 - ETA: 3s - loss: 0.8824 - accuracy: 0.57 - ETA: 3s - loss: 0.8821 - accuracy: 0.57 - ETA: 3s - loss: 0.8822 - accuracy: 0.57 - ETA: 3s - loss: 0.8825 - accuracy: 0.57 - ETA: 3s - loss: 0.8826 - accuracy: 0.57 - ETA: 3s - loss: 0.8818 - accuracy: 0.57 - ETA: 3s - loss: 0.8818 - accuracy: 0.57 - ETA: 3s - loss: 0.8816 - accuracy: 0.57 - ETA: 3s - loss: 0.8817 - accuracy: 0.57 - ETA: 3s - loss: 0.8820 - accuracy: 0.57 - ETA: 3s - loss: 0.8816 - accuracy: 0.57 - ETA: 3s - loss: 0.8812 - accuracy: 0.57 - ETA: 3s - loss: 0.8811 - accuracy: 0.57 - ETA: 3s - loss: 0.8816 - accuracy: 0.57 - ETA: 3s - loss: 0.8810 - accuracy: 0.57 - ETA: 3s - loss: 0.8805 - accuracy: 0.57 - ETA: 3s - loss: 0.8810 - accuracy: 0.57 - ETA: 3s - loss: 0.8813 - accuracy: 0.57 - ETA: 3s - loss: 0.8809 - accuracy: 0.57 - ETA: 2s - loss: 0.8805 - accuracy: 0.57 - ETA: 2s - loss: 0.8802 - accuracy: 0.57 - ETA: 2s - loss: 0.8807 - accuracy: 0.57 - ETA: 2s - loss: 0.8801 - accuracy: 0.57 - ETA: 2s - loss: 0.8802 - accuracy: 0.57 - ETA: 2s - loss: 0.8806 - accuracy: 0.57 - ETA: 2s - loss: 0.8807 - accuracy: 0.57 - ETA: 2s - loss: 0.8806 - accuracy: 0.57 - ETA: 2s - loss: 0.8806 - accuracy: 0.57 - ETA: 2s - loss: 0.8809 - accuracy: 0.57 - ETA: 2s - loss: 0.8813 - accuracy: 0.57 - ETA: 2s - loss: 0.8818 - accuracy: 0.57 - ETA: 2s - loss: 0.8826 - accuracy: 0.57 - ETA: 2s - loss: 0.8829 - accuracy: 0.57 - ETA: 2s - loss: 0.8823 - accuracy: 0.57 - ETA: 2s - loss: 0.8820 - accuracy: 0.57 - ETA: 2s - loss: 0.8821 - accuracy: 0.57 - ETA: 2s - loss: 0.8819 - accuracy: 0.57 - ETA: 2s - loss: 0.8822 - accuracy: 0.57 - ETA: 1s - loss: 0.8819 - accuracy: 0.57 - ETA: 1s - loss: 0.8823 - accuracy: 0.57 - ETA: 1s - loss: 0.8823 - accuracy: 0.57 - ETA: 1s - loss: 0.8818 - accuracy: 0.57 - ETA: 1s - loss: 0.8820 - accuracy: 0.57 - ETA: 1s - loss: 0.8821 - accuracy: 0.57 - ETA: 1s - loss: 0.8822 - accuracy: 0.57 - ETA: 1s - loss: 0.8824 - accuracy: 0.57 - ETA: 1s - loss: 0.8824 - accuracy: 0.57 - ETA: 1s - loss: 0.8821 - accuracy: 0.57 - ETA: 1s - loss: 0.8826 - accuracy: 0.57 - ETA: 1s - loss: 0.8822 - accuracy: 0.57 - ETA: 0s - loss: 0.8819 - accuracy: 0.57 - ETA: 0s - loss: 0.8823 - accuracy: 0.57 - ETA: 0s - loss: 0.8819 - accuracy: 0.57 - ETA: 0s - loss: 0.8819 - accuracy: 0.57 - ETA: 0s - loss: 0.8818 - accuracy: 0.57 - ETA: 0s - loss: 0.8820 - accuracy: 0.57 - ETA: 0s - loss: 0.8821 - accuracy: 0.57 - ETA: 0s - loss: 0.8818 - accuracy: 0.57 - ETA: 0s - loss: 0.8820 - accuracy: 0.57 - ETA: 0s - loss: 0.8822 - accuracy: 0.57 - ETA: 0s - loss: 0.8819 - accuracy: 0.57 - ETA: 0s - loss: 0.8820 - accuracy: 0.57 - ETA: 0s - loss: 0.8820 - accuracy: 0.57 - ETA: 0s - loss: 0.8821 - accuracy: 0.57 - ETA: 0s - loss: 0.8819 - accuracy: 0.57 - ETA: 0s - loss: 0.8820 - accuracy: 0.57 - ETA: 0s - loss: 0.8821 - accuracy: 0.57 - ETA: 0s - loss: 0.8824 - accuracy: 0.57 - ETA: 0s - loss: 0.8825 - accuracy: 0.57 - ETA: 0s - loss: 0.8825 - accuracy: 0.57 - ETA: 0s - loss: 0.8828 - accuracy: 0.57 - ETA: 0s - loss: 0.8830 - accuracy: 0.57 - ETA: 0s - loss: 0.8829 - accuracy: 0.57 - ETA: 0s - loss: 0.8829 - accuracy: 0.57 - ETA: 0s - loss: 0.8827 - accuracy: 0.57 - ETA: 0s - loss: 0.8830 - accuracy: 0.57 - ETA: 0s - loss: 0.8829 - accuracy: 0.57 - ETA: 0s - loss: 0.8828 - accuracy: 0.57 - ETA: 0s - loss: 0.8825 - accuracy: 0.57 - ETA: 0s - loss: 0.8825 - accuracy: 0.57 - ETA: 0s - loss: 0.8821 - accuracy: 0.57 - 7s 273us/step - loss: 0.8820 - accuracy: 0.5755\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27200/27200 [==============================] - ETA: 16s - loss: 0.7497 - accuracy: 0.625 - ETA: 15s - loss: 0.8781 - accuracy: 0.544 - ETA: 13s - loss: 0.8546 - accuracy: 0.593 - ETA: 12s - loss: 0.9045 - accuracy: 0.567 - ETA: 12s - loss: 0.8860 - accuracy: 0.568 - ETA: 11s - loss: 0.8815 - accuracy: 0.565 - ETA: 12s - loss: 0.8851 - accuracy: 0.563 - ETA: 11s - loss: 0.8845 - accuracy: 0.568 - ETA: 11s - loss: 0.8785 - accuracy: 0.575 - ETA: 11s - loss: 0.8790 - accuracy: 0.575 - ETA: 11s - loss: 0.8747 - accuracy: 0.577 - ETA: 11s - loss: 0.8693 - accuracy: 0.578 - ETA: 11s - loss: 0.8708 - accuracy: 0.581 - ETA: 11s - loss: 0.8783 - accuracy: 0.578 - ETA: 11s - loss: 0.8900 - accuracy: 0.573 - ETA: 11s - loss: 0.8894 - accuracy: 0.574 - ETA: 11s - loss: 0.8916 - accuracy: 0.575 - ETA: 11s - loss: 0.8891 - accuracy: 0.575 - ETA: 11s - loss: 0.8873 - accuracy: 0.576 - ETA: 10s - loss: 0.8902 - accuracy: 0.574 - ETA: 10s - loss: 0.8885 - accuracy: 0.577 - ETA: 10s - loss: 0.8877 - accuracy: 0.581 - ETA: 10s - loss: 0.8930 - accuracy: 0.577 - ETA: 10s - loss: 0.8903 - accuracy: 0.578 - ETA: 10s - loss: 0.8870 - accuracy: 0.579 - ETA: 10s - loss: 0.8886 - accuracy: 0.578 - ETA: 10s - loss: 0.8926 - accuracy: 0.575 - ETA: 9s - loss: 0.8907 - accuracy: 0.576 - ETA: 9s - loss: 0.8899 - accuracy: 0.57 - ETA: 9s - loss: 0.8891 - accuracy: 0.57 - ETA: 8s - loss: 0.8871 - accuracy: 0.57 - ETA: 8s - loss: 0.8816 - accuracy: 0.58 - ETA: 7s - loss: 0.8826 - accuracy: 0.58 - ETA: 7s - loss: 0.8856 - accuracy: 0.58 - ETA: 7s - loss: 0.8880 - accuracy: 0.57 - ETA: 7s - loss: 0.8876 - accuracy: 0.57 - ETA: 6s - loss: 0.8848 - accuracy: 0.57 - ETA: 6s - loss: 0.8849 - accuracy: 0.57 - ETA: 6s - loss: 0.8855 - accuracy: 0.57 - ETA: 6s - loss: 0.8823 - accuracy: 0.58 - ETA: 5s - loss: 0.8811 - accuracy: 0.58 - ETA: 5s - loss: 0.8836 - accuracy: 0.58 - ETA: 5s - loss: 0.8841 - accuracy: 0.58 - ETA: 5s - loss: 0.8840 - accuracy: 0.58 - ETA: 5s - loss: 0.8852 - accuracy: 0.57 - ETA: 4s - loss: 0.8846 - accuracy: 0.57 - ETA: 4s - loss: 0.8854 - accuracy: 0.57 - ETA: 4s - loss: 0.8858 - accuracy: 0.57 - ETA: 4s - loss: 0.8856 - accuracy: 0.57 - ETA: 4s - loss: 0.8853 - accuracy: 0.57 - ETA: 4s - loss: 0.8843 - accuracy: 0.57 - ETA: 4s - loss: 0.8841 - accuracy: 0.57 - ETA: 3s - loss: 0.8863 - accuracy: 0.57 - ETA: 3s - loss: 0.8855 - accuracy: 0.57 - ETA: 3s - loss: 0.8854 - accuracy: 0.57 - ETA: 3s - loss: 0.8850 - accuracy: 0.57 - ETA: 3s - loss: 0.8852 - accuracy: 0.57 - ETA: 3s - loss: 0.8846 - accuracy: 0.57 - ETA: 3s - loss: 0.8831 - accuracy: 0.57 - ETA: 3s - loss: 0.8816 - accuracy: 0.57 - ETA: 3s - loss: 0.8806 - accuracy: 0.57 - ETA: 2s - loss: 0.8802 - accuracy: 0.57 - ETA: 2s - loss: 0.8791 - accuracy: 0.57 - ETA: 2s - loss: 0.8785 - accuracy: 0.57 - ETA: 2s - loss: 0.8787 - accuracy: 0.57 - ETA: 2s - loss: 0.8791 - accuracy: 0.57 - ETA: 2s - loss: 0.8790 - accuracy: 0.57 - ETA: 2s - loss: 0.8787 - accuracy: 0.57 - ETA: 2s - loss: 0.8786 - accuracy: 0.57 - ETA: 2s - loss: 0.8792 - accuracy: 0.57 - ETA: 2s - loss: 0.8789 - accuracy: 0.57 - ETA: 2s - loss: 0.8788 - accuracy: 0.57 - ETA: 2s - loss: 0.8786 - accuracy: 0.57 - ETA: 2s - loss: 0.8796 - accuracy: 0.57 - ETA: 1s - loss: 0.8804 - accuracy: 0.57 - ETA: 1s - loss: 0.8801 - accuracy: 0.57 - ETA: 1s - loss: 0.8803 - accuracy: 0.57 - ETA: 1s - loss: 0.8811 - accuracy: 0.57 - ETA: 1s - loss: 0.8810 - accuracy: 0.57 - ETA: 1s - loss: 0.8809 - accuracy: 0.57 - ETA: 1s - loss: 0.8811 - accuracy: 0.57 - ETA: 1s - loss: 0.8805 - accuracy: 0.57 - ETA: 1s - loss: 0.8808 - accuracy: 0.57 - ETA: 1s - loss: 0.8814 - accuracy: 0.57 - ETA: 1s - loss: 0.8808 - accuracy: 0.57 - ETA: 1s - loss: 0.8814 - accuracy: 0.57 - ETA: 0s - loss: 0.8817 - accuracy: 0.57 - ETA: 0s - loss: 0.8814 - accuracy: 0.57 - ETA: 0s - loss: 0.8817 - accuracy: 0.57 - ETA: 0s - loss: 0.8815 - accuracy: 0.57 - ETA: 0s - loss: 0.8811 - accuracy: 0.57 - ETA: 0s - loss: 0.8808 - accuracy: 0.57 - ETA: 0s - loss: 0.8802 - accuracy: 0.57 - ETA: 0s - loss: 0.8793 - accuracy: 0.57 - ETA: 0s - loss: 0.8788 - accuracy: 0.57 - ETA: 0s - loss: 0.8788 - accuracy: 0.57 - ETA: 0s - loss: 0.8783 - accuracy: 0.57 - ETA: 0s - loss: 0.8787 - accuracy: 0.57 - ETA: 0s - loss: 0.8788 - accuracy: 0.57 - ETA: 0s - loss: 0.8783 - accuracy: 0.57 - 5s 191us/step - loss: 0.8788 - accuracy: 0.5763\n",
      "6800/6800 [==============================] - ETA: 22 - ETA: 0 - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Accuracy: 56.53\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y2, test_size=0.2, random_state = 2)\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=17, activation='relu'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='sigmoid'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# compile the keras model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=16)\n",
    "#evaluate the keras model\n",
    "ass1=model.predict(x_test)\n",
    "_, accuracy = model.evaluate(x_test, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6800/6800 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 30us/step\n",
      "Accuracy: 54.10\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
